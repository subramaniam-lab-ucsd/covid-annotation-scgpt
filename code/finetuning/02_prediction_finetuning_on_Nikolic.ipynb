{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8dbd4e6",
   "metadata": {},
   "source": [
    "# Fine-tuning on Pre-trained Model for Cell-type Annotation in Nikolic et al\n",
    "\n",
    "\n",
    "     1. Specify hyper-parameter setup for integration task\n",
    "     \n",
    "     2. Load and pre-process data\n",
    "     \n",
    "     3. Load the pre-trained scGPT model\n",
    "     \n",
    "     4. Finetune scGPT with task-specific objectives\n",
    "     \n",
    "     5. Evaluate fine-tuned scGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9406b4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s5srinivasan/py39env/lib64/python3.9/site-packages/scgpt/model/model.py:21: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/home/s5srinivasan/py39env/lib64/python3.9/site-packages/scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/home/s5srinivasan/py39env/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "/home/s5srinivasan/py39env/lib64/python3.9/site-packages/scanpy/_settings.py:488: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  IPython.display.set_matplotlib_formats(*ipython_format)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import pandas as pd\n",
    "# from . import asyn\n",
    "import pickle\n",
    "import torch\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "# import scvi\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
    "sc.set_figure_params(figsize=(6, 6))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d22b3c",
   "metadata": {},
   "source": [
    "## Step1: Specify hyper-parameter setup for cell-type annotation task\n",
    "Listed below are some hyper-parameter recommendations for the cell-type task. Note that the CLS objective is on to facilitate cell-type classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc5c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_defaults = dict(\n",
    "    seed=0,\n",
    "    dataset_name=\"covid\",\n",
    "    do_train=True,\n",
    "    load_model=\"/home/s5srinivasan/covid-annotation-scgpt/save/scgpt-human\",\n",
    "    mask_ratio=0.0,\n",
    "    epochs=60,\n",
    "    n_bins=51,\n",
    "    MVC=False, # Masked value prediction for cell embedding\n",
    "    ecs_thres=0.0, # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=0.0,\n",
    "    lr=1e-5,\n",
    "    batch_size=18, #rtx3090 - 18, a100 - 32, v100 - 24, a30 - 18\n",
    "    layer_size=128,\n",
    "    nlayers=4,  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead=4,  # number of heads in nn.MultiheadAttention\n",
    "    dropout=0.2,  # dropout probability\n",
    "    schedule_ratio=0.9,  # ratio of epochs for learning rate schedule\n",
    "    save_eval_interval=5,\n",
    "    fast_transformer=True,\n",
    "    pre_norm=False,\n",
    "    amp=True,  # Automatic Mixed Precision\n",
    "    include_zero_gene = False,\n",
    "    freeze = False, #freeze\n",
    "    DSBN = False,  # Domain-spec batchnorm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f427b1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrks\u001b[0m (\u001b[33msrks-uc-san-diego\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/s5srinivasan/covid-annotation-scgpt/code/finetuning/wandb/run-20250224_222149-l5ao57rp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/srks-uc-san-diego/scGPT-covid-annotation/runs/l5ao57rp' target=\"_blank\">rural-firefly-104</a></strong> to <a href='https://wandb.ai/srks-uc-san-diego/scGPT-covid-annotation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/srks-uc-san-diego/scGPT-covid-annotation' target=\"_blank\">https://wandb.ai/srks-uc-san-diego/scGPT-covid-annotation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/srks-uc-san-diego/scGPT-covid-annotation/runs/l5ao57rp' target=\"_blank\">https://wandb.ai/srks-uc-san-diego/scGPT-covid-annotation/runs/l5ao57rp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 0, 'dataset_name': 'covid', 'do_train': True, 'load_model': '/home/s5srinivasan/covid-annotation-scgpt/save/scgpt-human', 'mask_ratio': 0.0, 'epochs': 30, 'n_bins': 51, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 1e-05, 'batch_size': 18, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': True, 'pre_norm': False, 'amp': True, 'include_zero_gene': False, 'freeze': False, 'DSBN': False}\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    config=hyperparameter_defaults,\n",
    "    project=\"scGPT-covid-annotation\",\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"fork\"),\n",
    ")\n",
    "config = wandb.config\n",
    "print(config)\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b568e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for input and preprocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_ratio = config.mask_ratio\n",
    "mask_value = \"auto\"  # for masked values, now it should always be auto\n",
    "\n",
    "include_zero_gene = config.include_zero_gene  # if True, include zero genes among hvgs in the training\n",
    "max_seq_len = 3001\n",
    "n_bins = config.n_bins\n",
    "\n",
    "# input/output representation\n",
    "input_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "output_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "\n",
    "# settings for training\n",
    "MLM = False  # whether to use masked language modeling, currently it is always on.\n",
    "CLS = True  # celltype classification objective\n",
    "ADV = False  # Adversarial training for batch correction\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "MVC = config.MVC  # Masked value prediction for cell embedding\n",
    "ECS = config.ecs_thres > 0  # Elastic cell similarity objective\n",
    "DAB = False  # Domain adaptation by reverse backpropagation, set to 2 for separate optimizer\n",
    "INPUT_BATCH_LABELS = False  # TODO: have these help MLM and MVC, while not to classifier\n",
    "input_emb_style = \"continuous\"  # \"category\" or \"continuous\" or \"scaling\"\n",
    "cell_emb_style = \"cls\"  # \"avg-pool\" or \"w-pool\" or \"cls\"\n",
    "adv_E_delay_epochs = 0  # delay adversarial training on encoder for a few epochs\n",
    "adv_D_delay_epochs = 0\n",
    "mvc_decoder_style = \"inner product\"\n",
    "ecs_threshold = config.ecs_thres\n",
    "dab_weight = config.dab_weight\n",
    "\n",
    "explicit_zero_prob = MLM and include_zero_gene  # whether explicit bernoulli for zeros\n",
    "do_sample_in_train = False and explicit_zero_prob  # sample the bernoulli in training\n",
    "\n",
    "per_seq_batch_sample = False\n",
    "\n",
    "# settings for optimizer\n",
    "lr = config.lr  # TODO: test learning rate ratio between two tasks\n",
    "lr_ADV = 1e-3  # learning rate for discriminator, used when ADV is True\n",
    "batch_size = config.batch_size\n",
    "eval_batch_size = config.batch_size\n",
    "epochs = config.epochs\n",
    "schedule_interval = 1\n",
    "\n",
    "# settings for the model\n",
    "fast_transformer = config.fast_transformer\n",
    "fast_transformer_backend = \"flash\"  # \"linear\" or \"flash\"\n",
    "embsize = config.layer_size  # embedding dimension\n",
    "d_hid = config.layer_size  # dimension of the feedforward network in TransformerEncoder\n",
    "nlayers = config.nlayers  # number of TransformerEncoderLayer in TransformerEncoder\n",
    "nhead = config.nhead  # number of heads in nn.MultiheadAttention\n",
    "dropout = config.dropout  # dropout probability\n",
    "\n",
    "# logging\n",
    "log_interval = 100  # iterations\n",
    "save_eval_interval = config.save_eval_interval  # epochs\n",
    "do_eval_scib_metrics = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d03a69bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% validate settings\n",
    "assert input_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert output_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert input_emb_style in [\"category\", \"continuous\", \"scaling\"]\n",
    "if input_style == \"binned\":\n",
    "    if input_emb_style == \"scaling\":\n",
    "        raise ValueError(\"input_emb_style `scaling` is not supported for binned input.\")\n",
    "elif input_style == \"log1p\" or input_style == \"normed_raw\":\n",
    "    if input_emb_style == \"category\":\n",
    "        raise ValueError(\n",
    "            \"input_emb_style `category` is not supported for log1p or normed_raw input.\"\n",
    "        )\n",
    "\n",
    "if input_emb_style == \"category\":\n",
    "    mask_value = n_bins + 1\n",
    "    pad_value = n_bins  # for padding gene expr values\n",
    "    n_input_bins = n_bins + 2\n",
    "else:\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = n_bins\n",
    "\n",
    "if ADV and DAB:\n",
    "    raise ValueError(\"ADV and DAB cannot be both True.\")\n",
    "DAB_separate_optim = True if DAB > 1 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0840de52",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = Path(f\"/home/s5srinivasan/covid-annotation-scgpt/\")\n",
    "data_dir = Path(f\"/home/s5srinivasan/covid-annotation-scgpt/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37ab046a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to /home/s5srinivasan/covid-annotation-scgpt/save/dev_covid-Feb24-22-21\n"
     ]
    }
   ],
   "source": [
    "dataset_name = config.dataset_name\n",
    "save_dir = Path(project_dir/f\"save/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"save to {save_dir}\")\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346bac9c",
   "metadata": {},
   "source": [
    "## Step 2: Load and pre-process data\n",
    "We follow the standard scGPT data pre-processing pipelines for the cell-type annotation task. Note that since now we have two datasets at hand (i.e., reference and query data), the same pre-prpocessing steps need to be applied to both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c76734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adataOriginal = sc.read(data_dir/\"nikolicPBMC.h5ad\")\n",
    "    # adata_test = sc.read(data_dir / \"covidData_SS_C1.h5ad\")\n",
    "    # adata.obs[\"celltype\"] = adata.obs[\"Factor Value[inferred cell type - authors labels]\"].astype(\"category\")\n",
    "    # adata_test.obs[\"celltype\"] = adata_test.obs[\"Factor Value[inferred cell type - authors labels]\"].astype(\"category\")\n",
    "    # adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = \"0\"\n",
    "    # adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"1\"          \n",
    "    # adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    # adata_test.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "\n",
    "data_is_raw = False\n",
    "filter_gene_by_counts = False\n",
    "    # adata = adata.concatenate(adata_test, batch_key=\"str_batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b7ac4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# countsOriginal = adataOriginal.obs.annotation_detailed.value_counts()\n",
    "\n",
    "# def valueCountsPlot(counts):\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     sns.barplot(x=counts.index, y=counts.values)\n",
    "\n",
    "#     # Rotate x-axis labels for readability\n",
    "#     plt.xticks(rotation=90)\n",
    "\n",
    "#     plt.xlabel(\"Annotation Type\")\n",
    "#     plt.ylabel(\"Number of Cells\")\n",
    "#     plt.title(\"Cell Counts per Annotation Type\")\n",
    "#     plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# valueCountsPlot(countsOriginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d3989e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.pp.subsample(adataOriginal, fraction=0.1, copy=True)\n",
    "# countsSubsampled = adata.obs.annotation_detailed.value_counts()\n",
    "\n",
    "# valueCountsPlot(countsSubsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83e3e5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "annotation_detailed\n",
       "T CD4 naive               7686\n",
       "Monocyte CD14             4719\n",
       "NK                        4328\n",
       "B naive                   3882\n",
       "T CD4 helper              3862\n",
       "T CD8 CTL                 3735\n",
       "T CD8 naive               3413\n",
       "T CD8 CM                  1231\n",
       "T reg                      887\n",
       "Monocyte CD16              872\n",
       "T CD4 naive IFN stim       792\n",
       "Monocyte CD14 IFN stim     703\n",
       "T CD8 EMRA                 597\n",
       "B n-sw mem                 568\n",
       "T CD4 CTL                  568\n",
       "T g/d                      538\n",
       "NK CD56                    453\n",
       "B sw mem                   397\n",
       "MAIT                       394\n",
       "Cycling                    357\n",
       "cDC2                       304\n",
       "T CD8 EM                   231\n",
       "B invar                    207\n",
       "B naive IFN stim           203\n",
       "Platelets                  184\n",
       "Plasma cells               153\n",
       "pDC                        136\n",
       "Monocyte CD16 IFN stim     135\n",
       "Monocyte CD16+C1           111\n",
       "NK IFN stim                109\n",
       "HPC                        107\n",
       "Monocyte CD14 IL6           92\n",
       "NKT                         72\n",
       "B n-sw mem IFN stim         72\n",
       "RBC                         35\n",
       "ILC                         33\n",
       "Plasmablasts                25\n",
       "T CD8 CTL IFN stim          15\n",
       "AS-DC                        7\n",
       "cDC1                         6\n",
       "HPC IFN stim                 2\n",
       "Baso/Eos                     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs.annotation_detailed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87ed993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs[\"celltype\"] = adata.obs[\"annotation_detailed\"] # create a celltype column to duplicate the detailed annotations in a new col\n",
    "adata.var.rename(columns={'name':'gene_name'},inplace=True)\n",
    "adata.var.set_index(adata.var[\"gene_name\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0353a2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>BMI</th>\n",
       "      <th>annotation_broad</th>\n",
       "      <th>annotation_detailed</th>\n",
       "      <th>annotation_detailed_fullNames</th>\n",
       "      <th>Age_group</th>\n",
       "      <th>COVID_severity</th>\n",
       "      <th>COVID_status</th>\n",
       "      <th>Group</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>sequencing_library</th>\n",
       "      <th>Protein_modality_weight</th>\n",
       "      <th>celltype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CV001_KM10202385-CV001_KM10202395_AAAGCAACACACTGCG-1</th>\n",
       "      <td>AN6</td>\n",
       "      <td>EUR</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>B</td>\n",
       "      <td>B n-sw mem</td>\n",
       "      <td>B non-switched mem</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Female</td>\n",
       "      <td>Non-smoker</td>\n",
       "      <td>AN6</td>\n",
       "      <td>CV001_KM10202385-CV001_KM10202395</td>\n",
       "      <td>0.627888</td>\n",
       "      <td>B n-sw mem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S8_CACCACTTCAGTGTTG-1</th>\n",
       "      <td>NP30</td>\n",
       "      <td>AFR</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>T CD4+</td>\n",
       "      <td>T CD4 naive</td>\n",
       "      <td>T CD4 naive</td>\n",
       "      <td>Child</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>Paediatric</td>\n",
       "      <td>Female</td>\n",
       "      <td>Non-smoker</td>\n",
       "      <td>NP30</td>\n",
       "      <td>CV001_KM9166548-CV001_KM9166575</td>\n",
       "      <td>0.485969</td>\n",
       "      <td>T CD4 naive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S11_GAACATCCAGATCGGA-1</th>\n",
       "      <td>AP12</td>\n",
       "      <td>EAS.EUR</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NK</td>\n",
       "      <td>NK</td>\n",
       "      <td>NK</td>\n",
       "      <td>Elderly</td>\n",
       "      <td>Severe</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Male</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>AP12</td>\n",
       "      <td>CV001_KM9294105-CV001_KM9294121</td>\n",
       "      <td>0.682514</td>\n",
       "      <td>NK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S11_GTCACGGAGTTTCCTT-1</th>\n",
       "      <td>AP12</td>\n",
       "      <td>EAS.EUR</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NK</td>\n",
       "      <td>NK</td>\n",
       "      <td>NK</td>\n",
       "      <td>Elderly</td>\n",
       "      <td>Severe</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Male</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>AP12</td>\n",
       "      <td>CV001_KM9294105-CV001_KM9294121</td>\n",
       "      <td>0.573830</td>\n",
       "      <td>NK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S13_ACGGCCATCTTGGGTA-1</th>\n",
       "      <td>PP1</td>\n",
       "      <td>AFR</td>\n",
       "      <td>17.4</td>\n",
       "      <td>Monocyte</td>\n",
       "      <td>Monocyte CD14</td>\n",
       "      <td>Classical monocyte</td>\n",
       "      <td>Infant</td>\n",
       "      <td>Severe</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>Paediatric</td>\n",
       "      <td>Male</td>\n",
       "      <td>Non-smoker</td>\n",
       "      <td>PP1</td>\n",
       "      <td>CV001_KM9294108-CV001_KM9294123</td>\n",
       "      <td>0.403451</td>\n",
       "      <td>Monocyte CD14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV001_KM10202391-CV001_KM10202401_TACCTATGTCTCATCC-1</th>\n",
       "      <td>AN2</td>\n",
       "      <td>EAS.SAS</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Platelets</td>\n",
       "      <td>Platelets</td>\n",
       "      <td>Platelets</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Female</td>\n",
       "      <td>Non-smoker</td>\n",
       "      <td>AN2</td>\n",
       "      <td>CV001_KM10202391-CV001_KM10202401</td>\n",
       "      <td>0.087479</td>\n",
       "      <td>Platelets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV001_KM10202389-CV001_KM10202399_CCTATTATCAATACCG-1</th>\n",
       "      <td>PP6</td>\n",
       "      <td>EUR</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>T CD4+</td>\n",
       "      <td>T CD4 naive</td>\n",
       "      <td>T CD4 naive</td>\n",
       "      <td>Neonate</td>\n",
       "      <td>Asymptomatic</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>Paediatric</td>\n",
       "      <td>Male</td>\n",
       "      <td>Non-smoker</td>\n",
       "      <td>PP6</td>\n",
       "      <td>CV001_KM10202389-CV001_KM10202399</td>\n",
       "      <td>0.659727</td>\n",
       "      <td>T CD4 naive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV001_KM10202390-CV001_KM10202400_TCTGAGATCTGTCCGT-1</th>\n",
       "      <td>AN2</td>\n",
       "      <td>EAS.SAS</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>T CD4+</td>\n",
       "      <td>T CD4 naive</td>\n",
       "      <td>T CD4 naive</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Female</td>\n",
       "      <td>Non-smoker</td>\n",
       "      <td>AN2</td>\n",
       "      <td>CV001_KM10202390-CV001_KM10202400</td>\n",
       "      <td>0.492045</td>\n",
       "      <td>T CD4 naive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S16_CGCTATCTCCATTCTA-1</th>\n",
       "      <td>PP3</td>\n",
       "      <td>EUR.SAS</td>\n",
       "      <td>30.6</td>\n",
       "      <td>T CD8+</td>\n",
       "      <td>T CD8 CTL</td>\n",
       "      <td>T CD8 CTL</td>\n",
       "      <td>Adolescent</td>\n",
       "      <td>Severe</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>Paediatric</td>\n",
       "      <td>Male</td>\n",
       "      <td>Non-smoker</td>\n",
       "      <td>PP3</td>\n",
       "      <td>CV001_KM9294111-CV001_KM9294126</td>\n",
       "      <td>0.461040</td>\n",
       "      <td>T CD8 CTL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S21_GCACATATCAACCATG-1</th>\n",
       "      <td>AP7</td>\n",
       "      <td>EAS</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>T CD4+</td>\n",
       "      <td>T CD4 naive</td>\n",
       "      <td>T CD4 naive</td>\n",
       "      <td>Elderly</td>\n",
       "      <td>Mild</td>\n",
       "      <td>Post-COVID-19</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Female</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>AP7-post</td>\n",
       "      <td>CV001_KM9166641-CV001_KM9166649</td>\n",
       "      <td>0.359617</td>\n",
       "      <td>T CD4 naive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42222 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   patient_id Ethnicity  \\\n",
       "CV001_KM10202385-CV001_KM10202395_AAAGCAACACACT...        AN6       EUR   \n",
       "S8_CACCACTTCAGTGTTG-1                                    NP30       AFR   \n",
       "S11_GAACATCCAGATCGGA-1                                   AP12   EAS.EUR   \n",
       "S11_GTCACGGAGTTTCCTT-1                                   AP12   EAS.EUR   \n",
       "S13_ACGGCCATCTTGGGTA-1                                    PP1       AFR   \n",
       "...                                                       ...       ...   \n",
       "CV001_KM10202391-CV001_KM10202401_TACCTATGTCTCA...        AN2   EAS.SAS   \n",
       "CV001_KM10202389-CV001_KM10202399_CCTATTATCAATA...        PP6       EUR   \n",
       "CV001_KM10202390-CV001_KM10202400_TCTGAGATCTGTC...        AN2   EAS.SAS   \n",
       "S16_CGCTATCTCCATTCTA-1                                    PP3   EUR.SAS   \n",
       "S21_GCACATATCAACCATG-1                                    AP7       EAS   \n",
       "\n",
       "                                                        BMI annotation_broad  \\\n",
       "CV001_KM10202385-CV001_KM10202395_AAAGCAACACACT...  Unknown                B   \n",
       "S8_CACCACTTCAGTGTTG-1                               Unknown           T CD4+   \n",
       "S11_GAACATCCAGATCGGA-1                              Unknown               NK   \n",
       "S11_GTCACGGAGTTTCCTT-1                              Unknown               NK   \n",
       "S13_ACGGCCATCTTGGGTA-1                                 17.4         Monocyte   \n",
       "...                                                     ...              ...   \n",
       "CV001_KM10202391-CV001_KM10202401_TACCTATGTCTCA...  Unknown        Platelets   \n",
       "CV001_KM10202389-CV001_KM10202399_CCTATTATCAATA...  Unknown           T CD4+   \n",
       "CV001_KM10202390-CV001_KM10202400_TCTGAGATCTGTC...  Unknown           T CD4+   \n",
       "S16_CGCTATCTCCATTCTA-1                                 30.6           T CD8+   \n",
       "S21_GCACATATCAACCATG-1                              Unknown           T CD4+   \n",
       "\n",
       "                                                   annotation_detailed  \\\n",
       "CV001_KM10202385-CV001_KM10202395_AAAGCAACACACT...          B n-sw mem   \n",
       "S8_CACCACTTCAGTGTTG-1                                      T CD4 naive   \n",
       "S11_GAACATCCAGATCGGA-1                                              NK   \n",
       "S11_GTCACGGAGTTTCCTT-1                                              NK   \n",
       "S13_ACGGCCATCTTGGGTA-1                                   Monocyte CD14   \n",
       "...                                                                ...   \n",
       "CV001_KM10202391-CV001_KM10202401_TACCTATGTCTCA...           Platelets   \n",
       "CV001_KM10202389-CV001_KM10202399_CCTATTATCAATA...         T CD4 naive   \n",
       "CV001_KM10202390-CV001_KM10202400_TCTGAGATCTGTC...         T CD4 naive   \n",
       "S16_CGCTATCTCCATTCTA-1                                       T CD8 CTL   \n",
       "S21_GCACATATCAACCATG-1                                     T CD4 naive   \n",
       "\n",
       "                                                   annotation_detailed_fullNames  \\\n",
       "CV001_KM10202385-CV001_KM10202395_AAAGCAACACACT...            B non-switched mem   \n",
       "S8_CACCACTTCAGTGTTG-1                                                T CD4 naive   \n",
       "S11_GAACATCCAGATCGGA-1                                                        NK   \n",
       "S11_GTCACGGAGTTTCCTT-1                                                        NK   \n",
       "S13_ACGGCCATCTTGGGTA-1                                        Classical monocyte   \n",
       "...                                                                          ...   \n",
       "CV001_KM10202391-CV001_KM10202401_TACCTATGTCTCA...                     Platelets   \n",
       "CV001_KM10202389-CV001_KM10202399_CCTATTATCAATA...                   T CD4 naive   \n",
       "CV001_KM10202390-CV001_KM10202400_TCTGAGATCTGTC...                   T CD4 naive   \n",
       "S16_CGCTATCTCCATTCTA-1                                                 T CD8 CTL   \n",
       "S21_GCACATATCAACCATG-1                                               T CD4 naive   \n",
       "\n",
       "                                                     Age_group COVID_severity  \\\n",
       "CV001_KM10202385-CV001_KM10202395_AAAGCAACACACT...       Adult        Healthy   \n",
       "S8_CACCACTTCAGTGTTG-1                                    Child        Healthy   \n",
       "S11_GAACATCCAGATCGGA-1                                 Elderly         Severe   \n",
       "S11_GTCACGGAGTTTCCTT-1                                 Elderly         Severe   \n",
       "S13_ACGGCCATCTTGGGTA-1                                  Infant         Severe   \n",
       "...                                                        ...            ...   \n",
       "CV001_KM10202391-CV001_KM10202401_TACCTATGTCTCA...       Adult        Healthy   \n",
       "CV001_KM10202389-CV001_KM10202399_CCTATTATCAATA...     Neonate   Asymptomatic   \n",
       "CV001_KM10202390-CV001_KM10202400_TCTGAGATCTGTC...       Adult        Healthy   \n",
       "S16_CGCTATCTCCATTCTA-1                              Adolescent         Severe   \n",
       "S21_GCACATATCAACCATG-1                                 Elderly           Mild   \n",
       "\n",
       "                                                     COVID_status       Group  \\\n",
       "CV001_KM10202385-CV001_KM10202395_AAAGCAACACACT...        Healthy       Adult   \n",
       "S8_CACCACTTCAGTGTTG-1                                     Healthy  Paediatric   \n",
       "S11_GAACATCCAGATCGGA-1                                   COVID-19       Adult   \n",
       "S11_GTCACGGAGTTTCCTT-1                                   COVID-19       Adult   \n",
       "S13_ACGGCCATCTTGGGTA-1                                   COVID-19  Paediatric   \n",
       "...                                                           ...         ...   \n",
       "CV001_KM10202391-CV001_KM10202401_TACCTATGTCTCA...        Healthy       Adult   \n",
       "CV001_KM10202389-CV001_KM10202399_CCTATTATCAATA...       COVID-19  Paediatric   \n",
       "CV001_KM10202390-CV001_KM10202400_TCTGAGATCTGTC...        Healthy       Adult   \n",
       "S16_CGCTATCTCCATTCTA-1                                   COVID-19  Paediatric   \n",
       "S21_GCACATATCAACCATG-1                              Post-COVID-19       Adult   \n",
       "\n",
       "                                                       Sex      Smoker  \\\n",
       "CV001_KM10202385-CV001_KM10202395_AAAGCAACACACT...  Female  Non-smoker   \n",
       "S8_CACCACTTCAGTGTTG-1                               Female  Non-smoker   \n",
       "S11_GAACATCCAGATCGGA-1                                Male     Unknown   \n",
       "S11_GTCACGGAGTTTCCTT-1                                Male     Unknown   \n",
       "S13_ACGGCCATCTTGGGTA-1                                Male  Non-smoker   \n",
       "...                                                    ...         ...   \n",
       "CV001_KM10202391-CV001_KM10202401_TACCTATGTCTCA...  Female  Non-smoker   \n",
       "CV001_KM10202389-CV001_KM10202399_CCTATTATCAATA...    Male  Non-smoker   \n",
       "CV001_KM10202390-CV001_KM10202400_TCTGAGATCTGTC...  Female  Non-smoker   \n",
       "S16_CGCTATCTCCATTCTA-1                                Male  Non-smoker   \n",
       "S21_GCACATATCAACCATG-1                              Female   Ex-smoker   \n",
       "\n",
       "                                                   sample_id  \\\n",
       "CV001_KM10202385-CV001_KM10202395_AAAGCAACACACT...       AN6   \n",
       "S8_CACCACTTCAGTGTTG-1                                   NP30   \n",
       "S11_GAACATCCAGATCGGA-1                                  AP12   \n",
       "S11_GTCACGGAGTTTCCTT-1                                  AP12   \n",
       "S13_ACGGCCATCTTGGGTA-1                                   PP1   \n",
       "...                                                      ...   \n",
       "CV001_KM10202391-CV001_KM10202401_TACCTATGTCTCA...       AN2   \n",
       "CV001_KM10202389-CV001_KM10202399_CCTATTATCAATA...       PP6   \n",
       "CV001_KM10202390-CV001_KM10202400_TCTGAGATCTGTC...       AN2   \n",
       "S16_CGCTATCTCCATTCTA-1                                   PP3   \n",
       "S21_GCACATATCAACCATG-1                              AP7-post   \n",
       "\n",
       "                                                                   sequencing_library  \\\n",
       "CV001_KM10202385-CV001_KM10202395_AAAGCAACACACT...  CV001_KM10202385-CV001_KM10202395   \n",
       "S8_CACCACTTCAGTGTTG-1                                 CV001_KM9166548-CV001_KM9166575   \n",
       "S11_GAACATCCAGATCGGA-1                                CV001_KM9294105-CV001_KM9294121   \n",
       "S11_GTCACGGAGTTTCCTT-1                                CV001_KM9294105-CV001_KM9294121   \n",
       "S13_ACGGCCATCTTGGGTA-1                                CV001_KM9294108-CV001_KM9294123   \n",
       "...                                                                               ...   \n",
       "CV001_KM10202391-CV001_KM10202401_TACCTATGTCTCA...  CV001_KM10202391-CV001_KM10202401   \n",
       "CV001_KM10202389-CV001_KM10202399_CCTATTATCAATA...  CV001_KM10202389-CV001_KM10202399   \n",
       "CV001_KM10202390-CV001_KM10202400_TCTGAGATCTGTC...  CV001_KM10202390-CV001_KM10202400   \n",
       "S16_CGCTATCTCCATTCTA-1                                CV001_KM9294111-CV001_KM9294126   \n",
       "S21_GCACATATCAACCATG-1                                CV001_KM9166641-CV001_KM9166649   \n",
       "\n",
       "                                                    Protein_modality_weight  \\\n",
       "CV001_KM10202385-CV001_KM10202395_AAAGCAACACACT...                 0.627888   \n",
       "S8_CACCACTTCAGTGTTG-1                                              0.485969   \n",
       "S11_GAACATCCAGATCGGA-1                                             0.682514   \n",
       "S11_GTCACGGAGTTTCCTT-1                                             0.573830   \n",
       "S13_ACGGCCATCTTGGGTA-1                                             0.403451   \n",
       "...                                                                     ...   \n",
       "CV001_KM10202391-CV001_KM10202401_TACCTATGTCTCA...                 0.087479   \n",
       "CV001_KM10202389-CV001_KM10202399_CCTATTATCAATA...                 0.659727   \n",
       "CV001_KM10202390-CV001_KM10202400_TCTGAGATCTGTC...                 0.492045   \n",
       "S16_CGCTATCTCCATTCTA-1                                             0.461040   \n",
       "S21_GCACATATCAACCATG-1                                             0.359617   \n",
       "\n",
       "                                                         celltype  \n",
       "CV001_KM10202385-CV001_KM10202395_AAAGCAACACACT...     B n-sw mem  \n",
       "S8_CACCACTTCAGTGTTG-1                                 T CD4 naive  \n",
       "S11_GAACATCCAGATCGGA-1                                         NK  \n",
       "S11_GTCACGGAGTTTCCTT-1                                         NK  \n",
       "S13_ACGGCCATCTTGGGTA-1                              Monocyte CD14  \n",
       "...                                                           ...  \n",
       "CV001_KM10202391-CV001_KM10202401_TACCTATGTCTCA...      Platelets  \n",
       "CV001_KM10202389-CV001_KM10202399_CCTATTATCAATA...    T CD4 naive  \n",
       "CV001_KM10202390-CV001_KM10202400_TCTGAGATCTGTC...    T CD4 naive  \n",
       "S16_CGCTATCTCCATTCTA-1                                  T CD8 CTL  \n",
       "S21_GCACATATCAACCATG-1                                T CD4 naive  \n",
       "\n",
       "[42222 rows x 16 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5111111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "celltype_id_labels = adata.obs[\"celltype\"].astype(\"category\").cat.codes.values\n",
    "celltypes = adata.obs[\"celltype\"].unique()\n",
    "num_types = len(np.unique(celltype_id_labels))\n",
    "id2type = dict(enumerate(adata.obs[\"celltype\"].astype(\"category\").cat.categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cabd8bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs[\"celltype_id\"] = adata.obs[\"celltype\"].astype(\"category\").cat.codes\n",
    "adata.obs[\"batch_id\"] = adata.obs[\"sample_id\"].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6669196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/s5srinivasan/covid-annotation-scgpt/save/scgpt-human\n"
     ]
    }
   ],
   "source": [
    "print(config.load_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3cb0da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 23325/33751 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from /home/s5srinivasan/covid-annotation-scgpt/save/scgpt-human/best_model.pt, the model args will override the config /home/s5srinivasan/covid-annotation-scgpt/save/scgpt-human/args.json.\n"
     ]
    }
   ],
   "source": [
    "if config.load_model is not None:\n",
    "    model_dir = Path(config.load_model)\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    shutil.copy(vocab_file, save_dir / \"vocab.json\")\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will override the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c984395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the preprocessor, use the args to config the workflow\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=filter_gene_by_counts,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "168c1ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "# Randomly split data into 90% train and 10% test\n",
    "train_indices, test_indices = train_test_split(\n",
    "    adata.obs.index, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Create training and testing sets\n",
    "adata_train = adata[train_indices].copy()\n",
    "adata_test = adata[test_indices].copy()\n",
    "\n",
    "# Create a raw copy of the test set for predictions\n",
    "adata_test_raw = adata_test.copy()\n",
    "\n",
    "# Convert necessary columns to categorical\n",
    "adata_train.obs['celltype_id'] = adata_train.obs['celltype_id'].astype('category')\n",
    "adata_train.obs['batch_id'] = adata_train.obs['batch_id'].astype('category')\n",
    "adata_test.obs['batch_id'] = adata_test.obs['batch_id'].astype('category')\n",
    "adata_test_raw.obs['batch_id'] = adata_test_raw.obs['batch_id'].astype('category')\n",
    "\n",
    "# Preprocess both training and test sets\n",
    "preprocessor(adata_train, batch_key=None)\n",
    "preprocessor(adata_test, batch_key=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67ca8c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_key = {  # the values of this map coorespond to the keys in preprocessing\n",
    "    \"normed_raw\": \"X_normed\",\n",
    "    \"log1p\": \"X_normed\",\n",
    "    \"binned\": \"X_binned\",\n",
    "}[input_style]\n",
    "all_counts = (\n",
    "    adata_train.layers[input_layer_key].A\n",
    "    if issparse(adata_train.layers[input_layer_key])\n",
    "    else adata_train.layers[input_layer_key]\n",
    ")\n",
    "genes = adata_train.var[\"gene_name\"].tolist()\n",
    "\n",
    "celltypes_labels = adata_train.obs[\"celltype_id\"].tolist()  # make sure count from 0\n",
    "celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "batch_ids = adata_train.obs[\"batch_id\"].tolist()\n",
    "num_batch_types = len(set(batch_ids))\n",
    "batch_ids = np.array(batch_ids)\n",
    "\n",
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_celltype_labels,\n",
    "    valid_celltype_labels,\n",
    "    train_batch_labels,\n",
    "    valid_batch_labels,\n",
    ") = train_test_split(\n",
    "    all_counts, celltypes_labels, batch_ids, test_size=0.1, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9909c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cdaa765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - train set number of samples: 34199, \n",
      "\t feature length: 3001\n",
      "scGPT - INFO - valid set number of samples: 3800, \n",
      "\t feature length: 3001\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "logger.info(\n",
    "    f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14ceaf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sort_seq_batch=False) -> Tuple[Dict[str, torch.Tensor]]:\n",
    "    masked_values_train = random_mask_value(\n",
    "        tokenized_train[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    masked_values_valid = random_mask_value(\n",
    "        tokenized_valid[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    print(\n",
    "        f\"random masking at epoch {epoch:3d}, ratio of masked values in train: \",\n",
    "        f\"{(masked_values_train == mask_value).sum() / (masked_values_train - pad_value).count_nonzero():.4f}\",\n",
    "    )\n",
    "\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "    input_values_train, input_values_valid = masked_values_train, masked_values_valid\n",
    "    target_values_train, target_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    tensor_batch_labels_train = torch.from_numpy(train_batch_labels).long()\n",
    "    tensor_batch_labels_valid = torch.from_numpy(valid_batch_labels).long()\n",
    "\n",
    "    tensor_celltype_labels_train = torch.from_numpy(train_celltype_labels).long()\n",
    "    tensor_celltype_labels_valid = torch.from_numpy(valid_celltype_labels).long()\n",
    "\n",
    "    if sort_seq_batch:  # TODO: update to random pick seq source in each traning batch\n",
    "        train_sort_ids = np.argsort(train_batch_labels)\n",
    "        input_gene_ids_train = input_gene_ids_train[train_sort_ids]\n",
    "        input_values_train = input_values_train[train_sort_ids]\n",
    "        target_values_train = target_values_train[train_sort_ids]\n",
    "        tensor_batch_labels_train = tensor_batch_labels_train[train_sort_ids]\n",
    "        tensor_celltype_labels_train = tensor_celltype_labels_train[train_sort_ids]\n",
    "\n",
    "        valid_sort_ids = np.argsort(valid_batch_labels)\n",
    "        input_gene_ids_valid = input_gene_ids_valid[valid_sort_ids]\n",
    "        input_values_valid = input_values_valid[valid_sort_ids]\n",
    "        target_values_valid = target_values_valid[valid_sort_ids]\n",
    "        tensor_batch_labels_valid = tensor_batch_labels_valid[valid_sort_ids]\n",
    "        tensor_celltype_labels_valid = tensor_celltype_labels_valid[valid_sort_ids]\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"target_values\": target_values_train,\n",
    "        \"batch_labels\": tensor_batch_labels_train,\n",
    "        \"celltype_labels\": tensor_celltype_labels_train,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"target_values\": target_values_valid,\n",
    "        \"batch_labels\": tensor_batch_labels_valid,\n",
    "        \"celltype_labels\": tensor_celltype_labels_valid,\n",
    "    }\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    intra_domain_shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    if num_workers == 0:\n",
    "        num_workers = min(len(os.sched_getaffinity(0)), batch_size // 2)\n",
    "\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    if per_seq_batch_sample:\n",
    "        # find the indices of samples in each seq batch\n",
    "        subsets = []\n",
    "        batch_labels_array = data_pt[\"batch_labels\"].numpy()\n",
    "        for batch_label in np.unique(batch_labels_array):\n",
    "            batch_indices = np.where(batch_labels_array == batch_label)[0].tolist()\n",
    "            subsets.append(batch_indices)\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=SubsetsBatchSampler(\n",
    "                subsets,\n",
    "                batch_size,\n",
    "                intra_subset_shuffle=intra_domain_shuffle,\n",
    "                inter_subset_shuffle=shuffle,\n",
    "                drop_last=drop_last,\n",
    "            ),\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5189eba3",
   "metadata": {},
   "source": [
    "## Step 3: Load the pre-trained scGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ab06ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.bias with shape torch.Size([1])\n",
      "--------------------\n",
      "name: encoder.embedding.weight\n",
      "--------------------\n",
      "name: encoder.enc_norm.weight\n",
      "--------------------\n",
      "name: encoder.enc_norm.bias\n",
      "--------------------\n",
      "name: value_encoder.linear1.weight\n",
      "--------------------\n",
      "name: value_encoder.linear1.bias\n",
      "--------------------\n",
      "name: value_encoder.linear2.weight\n",
      "--------------------\n",
      "name: value_encoder.linear2.bias\n",
      "--------------------\n",
      "name: value_encoder.norm.weight\n",
      "--------------------\n",
      "name: value_encoder.norm.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.norm2.bias\n",
      "--------------------\n",
      "name: decoder.fc.0.weight\n",
      "--------------------\n",
      "name: decoder.fc.0.bias\n",
      "--------------------\n",
      "name: decoder.fc.2.weight\n",
      "--------------------\n",
      "name: decoder.fc.2.bias\n",
      "--------------------\n",
      "name: decoder.fc.4.weight\n",
      "--------------------\n",
      "name: decoder.fc.4.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.0.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.0.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.2.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.2.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.3.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.3.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.5.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.5.bias\n",
      "--------------------\n",
      "name: cls_decoder.out_layer.weight\n",
      "--------------------\n",
      "name: cls_decoder.out_layer.bias\n",
      "scGPT - INFO - Total Pre freeze Params 51353131\n",
      "scGPT - INFO - Total Post freeze Params 51353131\n",
      "scGPT - INFO - Using 2 GPUs\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=3,\n",
    "    n_cls=num_types if CLS else 1,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=MVC,\n",
    "    do_dab=DAB,\n",
    "    use_batch_labels=INPUT_BATCH_LABELS,\n",
    "    num_batch_labels=num_batch_types,\n",
    "    domain_spec_batchnorm=config.DSBN,\n",
    "    input_emb_style=input_emb_style,\n",
    "    n_input_bins=n_input_bins,\n",
    "    cell_emb_style=cell_emb_style,\n",
    "    mvc_decoder_style=mvc_decoder_style,\n",
    "    ecs_threshold=ecs_threshold,\n",
    "    explicit_zero_prob=explicit_zero_prob,\n",
    "    use_fast_transformer=fast_transformer,\n",
    "    fast_transformer_backend=fast_transformer_backend,\n",
    "    pre_norm=config.pre_norm,\n",
    ")\n",
    "if config.load_model is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        logger.info(f\"Loading all model params from {model_file}\")\n",
    "    except:\n",
    "        # only load params that are in the model and match the size\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file, map_location=torch.device('cpu'))\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        for k, v in pretrained_dict.items():\n",
    "            logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "pre_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "# Freeze all pre-decoder weights\n",
    "for name, para in model.named_parameters():\n",
    "    print(\"-\"*20)\n",
    "    print(f\"name: {name}\")\n",
    "    if config.freeze and \"encoder\" in name and \"transformer_encoder\" not in name:\n",
    "    # if config.freeze and \"encoder\" in name:\n",
    "        print(f\"freezing weights for: {name}\")\n",
    "        para.requires_grad = False\n",
    "\n",
    "post_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "logger.info(f\"Total Pre freeze Params {(pre_freeze_param_count )}\")\n",
    "logger.info(f\"Total Post freeze Params {(post_freeze_param_count )}\")\n",
    "wandb.log(\n",
    "        {\n",
    "            \"info/pre_freeze_param_count\": pre_freeze_param_count,\n",
    "            \"info/post_freeze_param_count\": post_freeze_param_count,\n",
    "        },\n",
    ")\n",
    "\n",
    "# **Wrap model in DataParallel to use multiple GPUs**\n",
    "device_indices = list(range(0,torch.cuda.device_count()))\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model, device_ids=device_indices)  # **Use the first two GPUs**\n",
    "    logger.info(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    \n",
    "model.to(device)\n",
    "wandb.watch(model)\n",
    "\n",
    "if ADV:\n",
    "    discriminator = AdversarialDiscriminator(\n",
    "        d_model=embsize,\n",
    "        n_cls=num_batch_types,\n",
    "    ).to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        discriminator = nn.DataParallel(discriminator, device_ids=device_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90cd613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = masked_mse_loss\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_dab = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=lr, eps=1e-4 if config.amp else 1e-8\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, schedule_interval, gamma=config.schedule_ratio\n",
    ")\n",
    "if DAB_separate_optim:\n",
    "    optimizer_dab = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler_dab = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_dab, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "if ADV:\n",
    "    criterion_adv = nn.CrossEntropyLoss()  # consider using label smoothing\n",
    "    optimizer_E = torch.optim.Adam(model.parameters(), lr=lr_ADV)\n",
    "    scheduler_E = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_E, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_ADV)\n",
    "    scheduler_D = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_D, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b77a5df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    (\n",
    "        total_loss,\n",
    "        total_mse,\n",
    "        total_cls,\n",
    "        total_cce,\n",
    "        total_mvc,\n",
    "        total_ecs,\n",
    "        total_dab,\n",
    "        total_adv_E,\n",
    "        total_adv_D,\n",
    "        total_zero_log_prob,\n",
    "        total_mvc_zero_log_prob,\n",
    "    ) = (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
    "    total_error = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        target_values = batch_data[\"target_values\"].to(device)\n",
    "        batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "        celltype_labels = batch_data[\"celltype_labels\"].to(device)\n",
    "\n",
    "        src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                CLS=CLS,\n",
    "                CCE=CCE,\n",
    "                MVC=MVC,\n",
    "                ECS=ECS,\n",
    "                do_sample=do_sample_in_train,\n",
    "                #generative_training=False\n",
    "            )\n",
    "\n",
    "            masked_positions = input_values.eq(mask_value)  # the postions to predict\n",
    "            loss = 0.0\n",
    "            metrics_to_log = {}\n",
    "            if MLM:\n",
    "                loss_mse = criterion(\n",
    "                    output_dict[\"mlm_output\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_mse\n",
    "                metrics_to_log = {\"train/mse\": loss_mse.item()}\n",
    "            if explicit_zero_prob:\n",
    "                loss_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mlm_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_zero_log_prob\n",
    "                metrics_to_log.update({\"train/nzlp\": loss_zero_log_prob.item()})\n",
    "            if CLS:\n",
    "                loss_cls = criterion_cls(output_dict[\"cls_output\"], celltype_labels)\n",
    "                loss = loss + loss_cls\n",
    "                metrics_to_log.update({\"train/cls\": loss_cls.item()})\n",
    "\n",
    "                error_rate = 1 - (\n",
    "                    (output_dict[\"cls_output\"].argmax(1) == celltype_labels)\n",
    "                    .sum()\n",
    "                    .item()\n",
    "                ) / celltype_labels.size(0)\n",
    "            if CCE:\n",
    "                loss_cce = 10 * output_dict[\"loss_cce\"]\n",
    "                loss = loss + loss_cce\n",
    "                metrics_to_log.update({\"train/cce\": loss_cce.item()})\n",
    "            if MVC:\n",
    "                loss_mvc = criterion(\n",
    "                    output_dict[\"mvc_output\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_mvc\n",
    "                metrics_to_log.update({\"train/mvc\": loss_mvc.item()})\n",
    "            if MVC and explicit_zero_prob:\n",
    "                loss_mvc_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mvc_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_mvc_zero_log_prob\n",
    "                metrics_to_log.update({\"train/mvc_nzlp\": loss_mvc_zero_log_prob.item()})\n",
    "            if ECS:\n",
    "                loss_ecs = 10 * output_dict[\"loss_ecs\"]\n",
    "                loss = loss + loss_ecs\n",
    "                metrics_to_log.update({\"train/ecs\": loss_ecs.item()})\n",
    "            if DAB:\n",
    "                # try weighting and separate optimizer\n",
    "                loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "                loss = loss + dab_weight * loss_dab\n",
    "                metrics_to_log.update({\"train/dab\": loss_dab.item()})\n",
    "\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.filterwarnings(\"always\")\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                1.0,\n",
    "                error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "            )\n",
    "            if len(w) > 0:\n",
    "                logger.warning(\n",
    "                    f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "                    f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "                    \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "                )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if ADV:\n",
    "            # rerun the model for adversarial training\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                CLS=CLS,\n",
    "                CCE=CCE,\n",
    "                MVC=MVC,\n",
    "                ECS=ECS,\n",
    "                do_sample=do_sample_in_train,\n",
    "                #generative_training=False\n",
    "            )\n",
    "\n",
    "            # TRAINING DISCRIMINATOR\n",
    "            loss_adv_D = criterion_adv(\n",
    "                discriminator(output_dict[\"cell_emb\"].detach()), batch_labels\n",
    "            )\n",
    "            if epoch > adv_D_delay_epochs:\n",
    "                discriminator.zero_grad()\n",
    "                loss_adv_D.backward()\n",
    "                optimizer_D.step()\n",
    "\n",
    "            # TRAINING ENCODER\n",
    "            loss_adv_E = -criterion_adv(\n",
    "                discriminator(output_dict[\"cell_emb\"]), batch_labels\n",
    "            )\n",
    "            # NOTE: the loss is negative here because we want to maximize\n",
    "            # the cross_entropy_loss, in other words, disguise against the discriminator\n",
    "            if epoch > adv_E_delay_epochs:\n",
    "                model.zero_grad()\n",
    "                discriminator.zero_grad()\n",
    "                loss_adv_E.backward()\n",
    "                optimizer_E.step()\n",
    "\n",
    "        wandb.log(metrics_to_log)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_mse += loss_mse.item() if MLM else 0.0\n",
    "        total_cls += loss_cls.item() if CLS else 0.0\n",
    "        total_cce += loss_cce.item() if CCE else 0.0\n",
    "        total_mvc += loss_mvc.item() if MVC else 0.0\n",
    "        total_ecs += loss_ecs.item() if ECS else 0.0\n",
    "        total_dab += loss_dab.item() if DAB else 0.0\n",
    "        total_adv_E += loss_adv_E.item() if ADV else 0.0\n",
    "        total_adv_D += loss_adv_D.item() if ADV else 0.0\n",
    "        total_zero_log_prob += loss_zero_log_prob.item() if explicit_zero_prob else 0.0\n",
    "        total_mvc_zero_log_prob += (\n",
    "            loss_mvc_zero_log_prob.item() if MVC and explicit_zero_prob else 0.0\n",
    "        )\n",
    "        total_error += error_rate\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            cur_mse = total_mse / log_interval\n",
    "            cur_cls = total_cls / log_interval if CLS else 0.0\n",
    "            cur_cce = total_cce / log_interval if CCE else 0.0\n",
    "            cur_mvc = total_mvc / log_interval if MVC else 0.0\n",
    "            cur_ecs = total_ecs / log_interval if ECS else 0.0\n",
    "            cur_dab = total_dab / log_interval if DAB else 0.0\n",
    "            cur_adv_E = total_adv_E / log_interval if ADV else 0.0\n",
    "            cur_adv_D = total_adv_D / log_interval if ADV else 0.0\n",
    "            cur_zero_log_prob = (\n",
    "                total_zero_log_prob / log_interval if explicit_zero_prob else 0.0\n",
    "            )\n",
    "            cur_mvc_zero_log_prob = (\n",
    "                total_mvc_zero_log_prob / log_interval\n",
    "                if MVC and explicit_zero_prob\n",
    "                else 0.0\n",
    "            )\n",
    "            cur_error = total_error / log_interval\n",
    "            # ppl = math.exp(cur_loss)\n",
    "            logger.info(\n",
    "                f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | \"\n",
    "                + (f\"mse {cur_mse:5.2f} | mre {cur_error:5.2f} |\" if MLM else \"\")\n",
    "                + (f\"cls {cur_cls:5.2f} | \" if CLS else \"\")\n",
    "                + (f\"err {cur_error:5.2f} | \" if CLS else \"\")\n",
    "                + (f\"cce {cur_cce:5.2f} |\" if CCE else \"\")\n",
    "                + (f\"mvc {cur_mvc:5.2f} |\" if MVC else \"\")\n",
    "                + (f\"ecs {cur_ecs:5.2f} |\" if ECS else \"\")\n",
    "                + (f\"dab {cur_dab:5.2f} |\" if DAB else \"\")\n",
    "                + (f\"adv_E {cur_adv_E:5.2f} |\" if ADV else \"\")\n",
    "                + (f\"adv_D {cur_adv_D:5.2f} |\" if ADV else \"\")\n",
    "                + (f\"nzlp {cur_zero_log_prob:5.2f} |\" if explicit_zero_prob else \"\")\n",
    "                + (\n",
    "                    f\"mvc_nzlp {cur_mvc_zero_log_prob:5.2f} |\"\n",
    "                    if MVC and explicit_zero_prob\n",
    "                    else \"\"\n",
    "                )\n",
    "            )\n",
    "            total_loss = 0\n",
    "            total_mse = 0\n",
    "            total_cls = 0\n",
    "            total_cce = 0\n",
    "            total_mvc = 0\n",
    "            total_ecs = 0\n",
    "            total_dab = 0\n",
    "            total_adv_E = 0\n",
    "            total_adv_D = 0\n",
    "            total_zero_log_prob = 0\n",
    "            total_mvc_zero_log_prob = 0\n",
    "            total_error = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def define_wandb_metrcis():\n",
    "    wandb.define_metric(\"valid/mse\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/mre\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/sum_mse_dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"test/avg_bio\", summary=\"max\")\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, return_raw: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    total_dab = 0.0\n",
    "    total_num = 0\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            target_values = batch_data[\"target_values\"].to(device)\n",
    "            batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "            celltype_labels = batch_data[\"celltype_labels\"].to(device)\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                    CLS=CLS,  # evaluation does not need CLS or CCE\n",
    "                    CCE=False,\n",
    "                    MVC=False,\n",
    "                    ECS=False,\n",
    "                    do_sample=do_sample_in_train,\n",
    "                    #generative_training = False,\n",
    "                )\n",
    "                output_values = output_dict[\"cls_output\"]\n",
    "                loss = criterion_cls(output_values, celltype_labels)\n",
    "\n",
    "                if DAB:\n",
    "                    loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "\n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            accuracy = (output_values.argmax(1) == celltype_labels).sum().item()\n",
    "            total_error += (1 - accuracy / len(input_gene_ids)) * len(input_gene_ids)\n",
    "            total_dab += loss_dab.item() * len(input_gene_ids) if DAB else 0.0\n",
    "            total_num += len(input_gene_ids)\n",
    "            preds = output_values.argmax(1).cpu().numpy()\n",
    "            predictions.append(preds)\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"valid/mse\": total_loss / total_num,\n",
    "            \"valid/err\": total_error / total_num,\n",
    "            \"valid/dab\": total_dab / total_num,\n",
    "            \"valid/sum_mse_dab\": (total_loss + dab_weight * total_dab) / total_num,\n",
    "            \"epoch\": epoch,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if return_raw:\n",
    "        return np.concatenate(predictions, axis=0)\n",
    "\n",
    "    return total_loss / total_num, total_error / total_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbd980fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.empty_cache() # clear GPU cache before finetuning task\n",
    "num_gpus = torch.cuda.device_count() # get visible GPU count\n",
    "print(f\"Number of available GPUs: {num_gpus}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d83dc7",
   "metadata": {},
   "source": [
    "## Step 4: Finetune scGPT with task-specific objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f70aad11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random masking at epoch   1, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   1 | 100/1900 batches | lr 0.0000 | ms/batch 583.36 | loss  3.07 | cls  3.07 | err  0.84 | \n",
      "scGPT - INFO - | epoch   1 | 200/1900 batches | lr 0.0000 | ms/batch 570.66 | loss  2.79 | cls  2.79 | err  0.80 | \n",
      "scGPT - INFO - | epoch   1 | 300/1900 batches | lr 0.0000 | ms/batch 575.88 | loss  2.74 | cls  2.74 | err  0.82 | \n",
      "scGPT - INFO - | epoch   1 | 400/1900 batches | lr 0.0000 | ms/batch 579.58 | loss  2.68 | cls  2.68 | err  0.74 | \n",
      "scGPT - INFO - | epoch   1 | 500/1900 batches | lr 0.0000 | ms/batch 578.88 | loss  2.32 | cls  2.32 | err  0.62 | \n",
      "scGPT - INFO - | epoch   1 | 600/1900 batches | lr 0.0000 | ms/batch 579.53 | loss  2.08 | cls  2.08 | err  0.53 | \n",
      "scGPT - INFO - | epoch   1 | 700/1900 batches | lr 0.0000 | ms/batch 579.83 | loss  1.88 | cls  1.88 | err  0.48 | \n",
      "scGPT - INFO - | epoch   1 | 800/1900 batches | lr 0.0000 | ms/batch 580.15 | loss  1.73 | cls  1.73 | err  0.45 | \n",
      "scGPT - INFO - | epoch   1 | 900/1900 batches | lr 0.0000 | ms/batch 580.39 | loss  1.66 | cls  1.66 | err  0.45 | \n",
      "scGPT - INFO - | epoch   1 | 1000/1900 batches | lr 0.0000 | ms/batch 583.06 | loss  1.53 | cls  1.53 | err  0.40 | \n",
      "scGPT - INFO - | epoch   1 | 1100/1900 batches | lr 0.0000 | ms/batch 580.40 | loss  1.54 | cls  1.54 | err  0.43 | \n",
      "scGPT - INFO - | epoch   1 | 1200/1900 batches | lr 0.0000 | ms/batch 580.36 | loss  1.51 | cls  1.51 | err  0.41 | \n",
      "scGPT - INFO - | epoch   1 | 1300/1900 batches | lr 0.0000 | ms/batch 580.55 | loss  1.46 | cls  1.46 | err  0.42 | \n",
      "scGPT - INFO - | epoch   1 | 1400/1900 batches | lr 0.0000 | ms/batch 580.55 | loss  1.40 | cls  1.40 | err  0.40 | \n",
      "scGPT - INFO - | epoch   1 | 1500/1900 batches | lr 0.0000 | ms/batch 580.76 | loss  1.43 | cls  1.43 | err  0.42 | \n",
      "scGPT - INFO - | epoch   1 | 1600/1900 batches | lr 0.0000 | ms/batch 582.13 | loss  1.42 | cls  1.42 | err  0.40 | \n",
      "scGPT - INFO - | epoch   1 | 1700/1900 batches | lr 0.0000 | ms/batch 580.73 | loss  1.31 | cls  1.31 | err  0.38 | \n",
      "scGPT - INFO - | epoch   1 | 1800/1900 batches | lr 0.0000 | ms/batch 580.41 | loss  1.21 | cls  1.21 | err  0.34 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   1 | time: 1150.04s | valid loss/mse 1.3230 | err 0.4139\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 1.3230\n",
      "random masking at epoch   2, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   2 | 100/1900 batches | lr 0.0000 | ms/batch 592.49 | loss  1.24 | cls  1.24 | err  0.35 | \n",
      "scGPT - INFO - | epoch   2 | 200/1900 batches | lr 0.0000 | ms/batch 582.44 | loss  1.21 | cls  1.21 | err  0.35 | \n",
      "scGPT - INFO - | epoch   2 | 300/1900 batches | lr 0.0000 | ms/batch 580.75 | loss  1.19 | cls  1.19 | err  0.36 | \n",
      "scGPT - INFO - | epoch   2 | 400/1900 batches | lr 0.0000 | ms/batch 580.77 | loss  1.17 | cls  1.17 | err  0.33 | \n",
      "scGPT - INFO - | epoch   2 | 500/1900 batches | lr 0.0000 | ms/batch 582.09 | loss  1.12 | cls  1.12 | err  0.31 | \n",
      "scGPT - INFO - | epoch   2 | 600/1900 batches | lr 0.0000 | ms/batch 580.86 | loss  1.09 | cls  1.09 | err  0.29 | \n",
      "scGPT - INFO - | epoch   2 | 700/1900 batches | lr 0.0000 | ms/batch 582.55 | loss  1.06 | cls  1.06 | err  0.29 | \n",
      "scGPT - INFO - | epoch   2 | 800/1900 batches | lr 0.0000 | ms/batch 580.89 | loss  1.08 | cls  1.08 | err  0.30 | \n",
      "scGPT - INFO - | epoch   2 | 900/1900 batches | lr 0.0000 | ms/batch 582.38 | loss  1.04 | cls  1.04 | err  0.29 | \n",
      "scGPT - INFO - | epoch   2 | 1000/1900 batches | lr 0.0000 | ms/batch 580.86 | loss  0.96 | cls  0.96 | err  0.27 | \n",
      "scGPT - INFO - | epoch   2 | 1100/1900 batches | lr 0.0000 | ms/batch 581.56 | loss  0.99 | cls  0.99 | err  0.28 | \n",
      "scGPT - INFO - | epoch   2 | 1200/1900 batches | lr 0.0000 | ms/batch 580.70 | loss  1.01 | cls  1.01 | err  0.28 | \n",
      "scGPT - INFO - | epoch   2 | 1300/1900 batches | lr 0.0000 | ms/batch 580.90 | loss  0.97 | cls  0.97 | err  0.28 | \n",
      "scGPT - INFO - | epoch   2 | 1400/1900 batches | lr 0.0000 | ms/batch 580.79 | loss  0.96 | cls  0.96 | err  0.28 | \n",
      "scGPT - INFO - | epoch   2 | 1500/1900 batches | lr 0.0000 | ms/batch 580.94 | loss  0.98 | cls  0.98 | err  0.28 | \n",
      "scGPT - INFO - | epoch   2 | 1600/1900 batches | lr 0.0000 | ms/batch 582.28 | loss  0.97 | cls  0.97 | err  0.27 | \n",
      "scGPT - INFO - | epoch   2 | 1700/1900 batches | lr 0.0000 | ms/batch 580.66 | loss  0.94 | cls  0.94 | err  0.27 | \n",
      "scGPT - INFO - | epoch   2 | 1800/1900 batches | lr 0.0000 | ms/batch 580.80 | loss  0.86 | cls  0.86 | err  0.24 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   2 | time: 1153.78s | valid loss/mse 0.9404 | err 0.2942\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.9404\n",
      "random masking at epoch   3, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   3 | 100/1900 batches | lr 0.0000 | ms/batch 592.11 | loss  0.88 | cls  0.88 | err  0.25 | \n",
      "scGPT - INFO - | epoch   3 | 200/1900 batches | lr 0.0000 | ms/batch 581.21 | loss  0.90 | cls  0.90 | err  0.26 | \n",
      "scGPT - INFO - | epoch   3 | 300/1900 batches | lr 0.0000 | ms/batch 580.48 | loss  0.82 | cls  0.82 | err  0.25 | \n",
      "scGPT - INFO - | epoch   3 | 400/1900 batches | lr 0.0000 | ms/batch 580.55 | loss  0.84 | cls  0.84 | err  0.25 | \n",
      "scGPT - INFO - | epoch   3 | 500/1900 batches | lr 0.0000 | ms/batch 582.43 | loss  0.86 | cls  0.86 | err  0.25 | \n",
      "scGPT - INFO - | epoch   3 | 600/1900 batches | lr 0.0000 | ms/batch 580.79 | loss  0.85 | cls  0.85 | err  0.25 | \n",
      "scGPT - INFO - | epoch   3 | 700/1900 batches | lr 0.0000 | ms/batch 580.99 | loss  0.83 | cls  0.83 | err  0.24 | \n",
      "scGPT - INFO - | epoch   3 | 800/1900 batches | lr 0.0000 | ms/batch 580.91 | loss  0.87 | cls  0.87 | err  0.26 | \n",
      "scGPT - INFO - | epoch   3 | 900/1900 batches | lr 0.0000 | ms/batch 580.89 | loss  0.83 | cls  0.83 | err  0.24 | \n",
      "scGPT - INFO - | epoch   3 | 1000/1900 batches | lr 0.0000 | ms/batch 580.70 | loss  0.79 | cls  0.79 | err  0.23 | \n",
      "scGPT - INFO - | epoch   3 | 1100/1900 batches | lr 0.0000 | ms/batch 582.04 | loss  0.81 | cls  0.81 | err  0.25 | \n",
      "scGPT - INFO - | epoch   3 | 1200/1900 batches | lr 0.0000 | ms/batch 581.59 | loss  0.84 | cls  0.84 | err  0.26 | \n",
      "scGPT - INFO - | epoch   3 | 1300/1900 batches | lr 0.0000 | ms/batch 580.77 | loss  0.79 | cls  0.79 | err  0.24 | \n",
      "scGPT - INFO - | epoch   3 | 1400/1900 batches | lr 0.0000 | ms/batch 580.75 | loss  0.80 | cls  0.80 | err  0.24 | \n",
      "scGPT - INFO - | epoch   3 | 1500/1900 batches | lr 0.0000 | ms/batch 580.81 | loss  0.81 | cls  0.81 | err  0.24 | \n",
      "scGPT - INFO - | epoch   3 | 1600/1900 batches | lr 0.0000 | ms/batch 580.92 | loss  0.82 | cls  0.82 | err  0.25 | \n",
      "scGPT - INFO - | epoch   3 | 1700/1900 batches | lr 0.0000 | ms/batch 581.00 | loss  0.76 | cls  0.76 | err  0.23 | \n",
      "scGPT - INFO - | epoch   3 | 1800/1900 batches | lr 0.0000 | ms/batch 582.16 | loss  0.72 | cls  0.72 | err  0.21 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   3 | time: 1153.30s | valid loss/mse 0.8486 | err 0.2718\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.8486\n",
      "random masking at epoch   4, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   4 | 100/1900 batches | lr 0.0000 | ms/batch 594.59 | loss  0.74 | cls  0.74 | err  0.22 | \n",
      "scGPT - INFO - | epoch   4 | 200/1900 batches | lr 0.0000 | ms/batch 581.14 | loss  0.77 | cls  0.77 | err  0.23 | \n",
      "scGPT - INFO - | epoch   4 | 300/1900 batches | lr 0.0000 | ms/batch 581.92 | loss  0.69 | cls  0.69 | err  0.22 | \n",
      "scGPT - INFO - | epoch   4 | 400/1900 batches | lr 0.0000 | ms/batch 580.91 | loss  0.73 | cls  0.73 | err  0.21 | \n",
      "scGPT - INFO - | epoch   4 | 500/1900 batches | lr 0.0000 | ms/batch 580.80 | loss  0.73 | cls  0.73 | err  0.22 | \n",
      "scGPT - INFO - | epoch   4 | 600/1900 batches | lr 0.0000 | ms/batch 581.00 | loss  0.71 | cls  0.71 | err  0.22 | \n",
      "scGPT - INFO - | epoch   4 | 700/1900 batches | lr 0.0000 | ms/batch 582.54 | loss  0.72 | cls  0.72 | err  0.21 | \n",
      "scGPT - INFO - | epoch   4 | 800/1900 batches | lr 0.0000 | ms/batch 580.93 | loss  0.76 | cls  0.76 | err  0.23 | \n",
      "scGPT - INFO - | epoch   4 | 900/1900 batches | lr 0.0000 | ms/batch 580.96 | loss  0.72 | cls  0.72 | err  0.22 | \n",
      "scGPT - INFO - | epoch   4 | 1000/1900 batches | lr 0.0000 | ms/batch 580.69 | loss  0.67 | cls  0.67 | err  0.20 | \n",
      "scGPT - INFO - | epoch   4 | 1100/1900 batches | lr 0.0000 | ms/batch 580.78 | loss  0.72 | cls  0.72 | err  0.22 | \n",
      "scGPT - INFO - | epoch   4 | 1200/1900 batches | lr 0.0000 | ms/batch 580.77 | loss  0.73 | cls  0.73 | err  0.23 | \n",
      "scGPT - INFO - | epoch   4 | 1300/1900 batches | lr 0.0000 | ms/batch 583.29 | loss  0.69 | cls  0.69 | err  0.22 | \n",
      "scGPT - INFO - | epoch   4 | 1400/1900 batches | lr 0.0000 | ms/batch 580.90 | loss  0.72 | cls  0.72 | err  0.22 | \n",
      "scGPT - INFO - | epoch   4 | 1500/1900 batches | lr 0.0000 | ms/batch 580.83 | loss  0.69 | cls  0.69 | err  0.21 | \n",
      "scGPT - INFO - | epoch   4 | 1600/1900 batches | lr 0.0000 | ms/batch 580.86 | loss  0.72 | cls  0.72 | err  0.22 | \n",
      "scGPT - INFO - | epoch   4 | 1700/1900 batches | lr 0.0000 | ms/batch 580.79 | loss  0.68 | cls  0.68 | err  0.20 | \n",
      "scGPT - INFO - | epoch   4 | 1800/1900 batches | lr 0.0000 | ms/batch 580.75 | loss  0.64 | cls  0.64 | err  0.19 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   4 | time: 1153.84s | valid loss/mse 0.7282 | err 0.2326\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.7282\n",
      "random masking at epoch   5, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   5 | 100/1900 batches | lr 0.0000 | ms/batch 591.80 | loss  0.65 | cls  0.65 | err  0.20 | \n",
      "scGPT - INFO - | epoch   5 | 200/1900 batches | lr 0.0000 | ms/batch 581.71 | loss  0.69 | cls  0.69 | err  0.22 | \n",
      "scGPT - INFO - | epoch   5 | 300/1900 batches | lr 0.0000 | ms/batch 580.04 | loss  0.61 | cls  0.61 | err  0.19 | \n",
      "scGPT - INFO - | epoch   5 | 400/1900 batches | lr 0.0000 | ms/batch 581.17 | loss  0.63 | cls  0.63 | err  0.20 | \n",
      "scGPT - INFO - | epoch   5 | 500/1900 batches | lr 0.0000 | ms/batch 581.69 | loss  0.63 | cls  0.63 | err  0.19 | \n",
      "scGPT - INFO - | epoch   5 | 600/1900 batches | lr 0.0000 | ms/batch 580.57 | loss  0.63 | cls  0.63 | err  0.19 | \n",
      "scGPT - INFO - | epoch   5 | 700/1900 batches | lr 0.0000 | ms/batch 580.53 | loss  0.64 | cls  0.64 | err  0.19 | \n",
      "scGPT - INFO - | epoch   5 | 800/1900 batches | lr 0.0000 | ms/batch 580.28 | loss  0.66 | cls  0.66 | err  0.21 | \n",
      "scGPT - INFO - | epoch   5 | 900/1900 batches | lr 0.0000 | ms/batch 581.72 | loss  0.65 | cls  0.65 | err  0.20 | \n",
      "scGPT - INFO - | epoch   5 | 1000/1900 batches | lr 0.0000 | ms/batch 580.15 | loss  0.59 | cls  0.59 | err  0.18 | \n",
      "scGPT - INFO - | epoch   5 | 1100/1900 batches | lr 0.0000 | ms/batch 580.12 | loss  0.64 | cls  0.64 | err  0.19 | \n",
      "scGPT - INFO - | epoch   5 | 1200/1900 batches | lr 0.0000 | ms/batch 581.54 | loss  0.65 | cls  0.65 | err  0.21 | \n",
      "scGPT - INFO - | epoch   5 | 1300/1900 batches | lr 0.0000 | ms/batch 580.21 | loss  0.61 | cls  0.61 | err  0.20 | \n",
      "scGPT - INFO - | epoch   5 | 1400/1900 batches | lr 0.0000 | ms/batch 581.17 | loss  0.63 | cls  0.63 | err  0.20 | \n",
      "scGPT - INFO - | epoch   5 | 1500/1900 batches | lr 0.0000 | ms/batch 580.20 | loss  0.62 | cls  0.62 | err  0.19 | \n",
      "scGPT - INFO - | epoch   5 | 1600/1900 batches | lr 0.0000 | ms/batch 581.60 | loss  0.64 | cls  0.64 | err  0.20 | \n",
      "scGPT - INFO - | epoch   5 | 1700/1900 batches | lr 0.0000 | ms/batch 580.27 | loss  0.60 | cls  0.60 | err  0.18 | \n",
      "scGPT - INFO - | epoch   5 | 1800/1900 batches | lr 0.0000 | ms/batch 580.24 | loss  0.58 | cls  0.58 | err  0.17 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   5 | time: 1152.74s | valid loss/mse 0.7056 | err 0.2263\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.7056\n",
      "random masking at epoch   6, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   6 | 100/1900 batches | lr 0.0000 | ms/batch 594.57 | loss  0.60 | cls  0.60 | err  0.19 | \n",
      "scGPT - INFO - | epoch   6 | 200/1900 batches | lr 0.0000 | ms/batch 581.00 | loss  0.63 | cls  0.63 | err  0.20 | \n",
      "scGPT - INFO - | epoch   6 | 300/1900 batches | lr 0.0000 | ms/batch 582.48 | loss  0.57 | cls  0.57 | err  0.18 | \n",
      "scGPT - INFO - | epoch   6 | 400/1900 batches | lr 0.0000 | ms/batch 580.58 | loss  0.58 | cls  0.58 | err  0.18 | \n",
      "scGPT - INFO - | epoch   6 | 500/1900 batches | lr 0.0000 | ms/batch 581.46 | loss  0.59 | cls  0.59 | err  0.18 | \n",
      "scGPT - INFO - | epoch   6 | 600/1900 batches | lr 0.0000 | ms/batch 581.79 | loss  0.59 | cls  0.59 | err  0.18 | \n",
      "scGPT - INFO - | epoch   6 | 700/1900 batches | lr 0.0000 | ms/batch 580.38 | loss  0.58 | cls  0.58 | err  0.18 | \n",
      "scGPT - INFO - | epoch   6 | 800/1900 batches | lr 0.0000 | ms/batch 581.79 | loss  0.61 | cls  0.61 | err  0.19 | \n",
      "scGPT - INFO - | epoch   6 | 900/1900 batches | lr 0.0000 | ms/batch 580.47 | loss  0.60 | cls  0.60 | err  0.19 | \n",
      "scGPT - INFO - | epoch   6 | 1000/1900 batches | lr 0.0000 | ms/batch 581.64 | loss  0.54 | cls  0.54 | err  0.17 | \n",
      "scGPT - INFO - | epoch   6 | 1100/1900 batches | lr 0.0000 | ms/batch 580.21 | loss  0.59 | cls  0.59 | err  0.18 | \n",
      "scGPT - INFO - | epoch   6 | 1200/1900 batches | lr 0.0000 | ms/batch 580.28 | loss  0.60 | cls  0.60 | err  0.20 | \n",
      "scGPT - INFO - | epoch   6 | 1300/1900 batches | lr 0.0000 | ms/batch 581.82 | loss  0.56 | cls  0.56 | err  0.19 | \n",
      "scGPT - INFO - | epoch   6 | 1400/1900 batches | lr 0.0000 | ms/batch 580.42 | loss  0.58 | cls  0.58 | err  0.19 | \n",
      "scGPT - INFO - | epoch   6 | 1500/1900 batches | lr 0.0000 | ms/batch 582.94 | loss  0.58 | cls  0.58 | err  0.18 | \n",
      "scGPT - INFO - | epoch   6 | 1600/1900 batches | lr 0.0000 | ms/batch 580.32 | loss  0.62 | cls  0.62 | err  0.20 | \n",
      "scGPT - INFO - | epoch   6 | 1700/1900 batches | lr 0.0000 | ms/batch 580.37 | loss  0.57 | cls  0.57 | err  0.18 | \n",
      "scGPT - INFO - | epoch   6 | 1800/1900 batches | lr 0.0000 | ms/batch 581.72 | loss  0.55 | cls  0.55 | err  0.17 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   6 | time: 1153.74s | valid loss/mse 0.6512 | err 0.2111\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.6512\n",
      "random masking at epoch   7, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   7 | 100/1900 batches | lr 0.0000 | ms/batch 592.08 | loss  0.56 | cls  0.56 | err  0.18 | \n",
      "scGPT - INFO - | epoch   7 | 200/1900 batches | lr 0.0000 | ms/batch 582.02 | loss  0.58 | cls  0.58 | err  0.19 | \n",
      "scGPT - INFO - | epoch   7 | 300/1900 batches | lr 0.0000 | ms/batch 580.14 | loss  0.53 | cls  0.53 | err  0.17 | \n",
      "scGPT - INFO - | epoch   7 | 400/1900 batches | lr 0.0000 | ms/batch 581.73 | loss  0.53 | cls  0.53 | err  0.17 | \n",
      "scGPT - INFO - | epoch   7 | 500/1900 batches | lr 0.0000 | ms/batch 580.37 | loss  0.54 | cls  0.54 | err  0.17 | \n",
      "scGPT - INFO - | epoch   7 | 600/1900 batches | lr 0.0000 | ms/batch 583.05 | loss  0.54 | cls  0.54 | err  0.17 | \n",
      "scGPT - INFO - | epoch   7 | 700/1900 batches | lr 0.0000 | ms/batch 580.61 | loss  0.55 | cls  0.55 | err  0.17 | \n",
      "scGPT - INFO - | epoch   7 | 800/1900 batches | lr 0.0000 | ms/batch 580.61 | loss  0.58 | cls  0.58 | err  0.18 | \n",
      "scGPT - INFO - | epoch   7 | 900/1900 batches | lr 0.0000 | ms/batch 581.99 | loss  0.56 | cls  0.56 | err  0.18 | \n",
      "scGPT - INFO - | epoch   7 | 1000/1900 batches | lr 0.0000 | ms/batch 580.36 | loss  0.50 | cls  0.50 | err  0.16 | \n",
      "scGPT - INFO - | epoch   7 | 1100/1900 batches | lr 0.0000 | ms/batch 581.81 | loss  0.55 | cls  0.55 | err  0.17 | \n",
      "scGPT - INFO - | epoch   7 | 1200/1900 batches | lr 0.0000 | ms/batch 580.45 | loss  0.56 | cls  0.56 | err  0.19 | \n",
      "scGPT - INFO - | epoch   7 | 1300/1900 batches | lr 0.0000 | ms/batch 580.58 | loss  0.52 | cls  0.52 | err  0.17 | \n",
      "scGPT - INFO - | epoch   7 | 1400/1900 batches | lr 0.0000 | ms/batch 581.88 | loss  0.56 | cls  0.56 | err  0.19 | \n",
      "scGPT - INFO - | epoch   7 | 1500/1900 batches | lr 0.0000 | ms/batch 580.55 | loss  0.56 | cls  0.56 | err  0.18 | \n",
      "scGPT - INFO - | epoch   7 | 1600/1900 batches | lr 0.0000 | ms/batch 582.88 | loss  0.58 | cls  0.58 | err  0.20 | \n",
      "scGPT - INFO - | epoch   7 | 1700/1900 batches | lr 0.0000 | ms/batch 580.39 | loss  0.54 | cls  0.54 | err  0.17 | \n",
      "scGPT - INFO - | epoch   7 | 1800/1900 batches | lr 0.0000 | ms/batch 581.80 | loss  0.51 | cls  0.51 | err  0.16 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   7 | time: 1153.63s | valid loss/mse 0.6034 | err 0.1913\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.6034\n",
      "random masking at epoch   8, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   8 | 100/1900 batches | lr 0.0000 | ms/batch 592.39 | loss  0.53 | cls  0.53 | err  0.17 | \n",
      "scGPT - INFO - | epoch   8 | 200/1900 batches | lr 0.0000 | ms/batch 580.79 | loss  0.55 | cls  0.55 | err  0.17 | \n",
      "scGPT - INFO - | epoch   8 | 300/1900 batches | lr 0.0000 | ms/batch 580.76 | loss  0.48 | cls  0.48 | err  0.16 | \n",
      "scGPT - INFO - | epoch   8 | 400/1900 batches | lr 0.0000 | ms/batch 580.81 | loss  0.51 | cls  0.51 | err  0.16 | \n",
      "scGPT - INFO - | epoch   8 | 500/1900 batches | lr 0.0000 | ms/batch 580.78 | loss  0.51 | cls  0.51 | err  0.16 | \n",
      "scGPT - INFO - | epoch   8 | 600/1900 batches | lr 0.0000 | ms/batch 582.72 | loss  0.52 | cls  0.52 | err  0.16 | \n",
      "scGPT - INFO - | epoch   8 | 700/1900 batches | lr 0.0000 | ms/batch 581.67 | loss  0.52 | cls  0.52 | err  0.17 | \n",
      "scGPT - INFO - | epoch   8 | 800/1900 batches | lr 0.0000 | ms/batch 580.94 | loss  0.55 | cls  0.55 | err  0.18 | \n",
      "scGPT - INFO - | epoch   8 | 900/1900 batches | lr 0.0000 | ms/batch 582.28 | loss  0.53 | cls  0.53 | err  0.17 | \n",
      "scGPT - INFO - | epoch   8 | 1000/1900 batches | lr 0.0000 | ms/batch 580.67 | loss  0.48 | cls  0.48 | err  0.16 | \n",
      "scGPT - INFO - | epoch   8 | 1100/1900 batches | lr 0.0000 | ms/batch 580.45 | loss  0.53 | cls  0.53 | err  0.17 | \n",
      "scGPT - INFO - | epoch   8 | 1200/1900 batches | lr 0.0000 | ms/batch 580.43 | loss  0.52 | cls  0.52 | err  0.17 | \n",
      "scGPT - INFO - | epoch   8 | 1300/1900 batches | lr 0.0000 | ms/batch 581.81 | loss  0.48 | cls  0.48 | err  0.17 | \n",
      "scGPT - INFO - | epoch   8 | 1400/1900 batches | lr 0.0000 | ms/batch 580.39 | loss  0.53 | cls  0.53 | err  0.17 | \n",
      "scGPT - INFO - | epoch   8 | 1500/1900 batches | lr 0.0000 | ms/batch 580.56 | loss  0.53 | cls  0.53 | err  0.17 | \n",
      "scGPT - INFO - | epoch   8 | 1600/1900 batches | lr 0.0000 | ms/batch 582.05 | loss  0.56 | cls  0.56 | err  0.18 | \n",
      "scGPT - INFO - | epoch   8 | 1700/1900 batches | lr 0.0000 | ms/batch 581.54 | loss  0.52 | cls  0.52 | err  0.16 | \n",
      "scGPT - INFO - | epoch   8 | 1800/1900 batches | lr 0.0000 | ms/batch 580.63 | loss  0.50 | cls  0.50 | err  0.16 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   8 | time: 1153.51s | valid loss/mse 0.5649 | err 0.1811\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.5649\n",
      "random masking at epoch   9, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   9 | 100/1900 batches | lr 0.0000 | ms/batch 594.05 | loss  0.51 | cls  0.51 | err  0.17 | \n",
      "scGPT - INFO - | epoch   9 | 200/1900 batches | lr 0.0000 | ms/batch 580.11 | loss  0.53 | cls  0.53 | err  0.17 | \n",
      "scGPT - INFO - | epoch   9 | 300/1900 batches | lr 0.0000 | ms/batch 580.56 | loss  0.47 | cls  0.47 | err  0.15 | \n",
      "scGPT - INFO - | epoch   9 | 400/1900 batches | lr 0.0000 | ms/batch 581.84 | loss  0.49 | cls  0.49 | err  0.15 | \n",
      "scGPT - INFO - | epoch   9 | 500/1900 batches | lr 0.0000 | ms/batch 580.36 | loss  0.48 | cls  0.48 | err  0.15 | \n",
      "scGPT - INFO - | epoch   9 | 600/1900 batches | lr 0.0000 | ms/batch 581.98 | loss  0.50 | cls  0.50 | err  0.16 | \n",
      "scGPT - INFO - | epoch   9 | 700/1900 batches | lr 0.0000 | ms/batch 580.83 | loss  0.50 | cls  0.50 | err  0.16 | \n",
      "scGPT - INFO - | epoch   9 | 800/1900 batches | lr 0.0000 | ms/batch 581.73 | loss  0.53 | cls  0.53 | err  0.17 | \n",
      "scGPT - INFO - | epoch   9 | 900/1900 batches | lr 0.0000 | ms/batch 582.01 | loss  0.51 | cls  0.51 | err  0.17 | \n",
      "scGPT - INFO - | epoch   9 | 1000/1900 batches | lr 0.0000 | ms/batch 580.67 | loss  0.46 | cls  0.46 | err  0.16 | \n",
      "scGPT - INFO - | epoch   9 | 1100/1900 batches | lr 0.0000 | ms/batch 581.94 | loss  0.51 | cls  0.51 | err  0.17 | \n",
      "scGPT - INFO - | epoch   9 | 1200/1900 batches | lr 0.0000 | ms/batch 580.62 | loss  0.50 | cls  0.50 | err  0.17 | \n",
      "scGPT - INFO - | epoch   9 | 1300/1900 batches | lr 0.0000 | ms/batch 582.17 | loss  0.47 | cls  0.47 | err  0.16 | \n",
      "scGPT - INFO - | epoch   9 | 1400/1900 batches | lr 0.0000 | ms/batch 580.62 | loss  0.50 | cls  0.50 | err  0.17 | \n",
      "scGPT - INFO - | epoch   9 | 1500/1900 batches | lr 0.0000 | ms/batch 580.67 | loss  0.51 | cls  0.51 | err  0.17 | \n",
      "scGPT - INFO - | epoch   9 | 1600/1900 batches | lr 0.0000 | ms/batch 581.90 | loss  0.54 | cls  0.54 | err  0.18 | \n",
      "scGPT - INFO - | epoch   9 | 1700/1900 batches | lr 0.0000 | ms/batch 580.64 | loss  0.50 | cls  0.50 | err  0.16 | \n",
      "scGPT - INFO - | epoch   9 | 1800/1900 batches | lr 0.0000 | ms/batch 582.75 | loss  0.48 | cls  0.48 | err  0.16 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   9 | time: 1153.90s | valid loss/mse 0.5468 | err 0.1787\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.5468\n",
      "random masking at epoch  10, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  10 | 100/1900 batches | lr 0.0000 | ms/batch 592.79 | loss  0.49 | cls  0.49 | err  0.15 | \n",
      "scGPT - INFO - | epoch  10 | 200/1900 batches | lr 0.0000 | ms/batch 581.07 | loss  0.51 | cls  0.51 | err  0.17 | \n",
      "scGPT - INFO - | epoch  10 | 300/1900 batches | lr 0.0000 | ms/batch 580.99 | loss  0.46 | cls  0.46 | err  0.15 | \n",
      "scGPT - INFO - | epoch  10 | 400/1900 batches | lr 0.0000 | ms/batch 580.97 | loss  0.48 | cls  0.48 | err  0.15 | \n",
      "scGPT - INFO - | epoch  10 | 500/1900 batches | lr 0.0000 | ms/batch 582.86 | loss  0.47 | cls  0.47 | err  0.15 | \n",
      "scGPT - INFO - | epoch  10 | 600/1900 batches | lr 0.0000 | ms/batch 581.02 | loss  0.49 | cls  0.49 | err  0.16 | \n",
      "scGPT - INFO - | epoch  10 | 700/1900 batches | lr 0.0000 | ms/batch 581.21 | loss  0.49 | cls  0.49 | err  0.16 | \n",
      "scGPT - INFO - | epoch  10 | 800/1900 batches | lr 0.0000 | ms/batch 581.09 | loss  0.52 | cls  0.52 | err  0.16 | \n",
      "scGPT - INFO - | epoch  10 | 900/1900 batches | lr 0.0000 | ms/batch 581.75 | loss  0.50 | cls  0.50 | err  0.16 | \n",
      "scGPT - INFO - | epoch  10 | 1000/1900 batches | lr 0.0000 | ms/batch 580.64 | loss  0.46 | cls  0.46 | err  0.15 | \n",
      "scGPT - INFO - | epoch  10 | 1100/1900 batches | lr 0.0000 | ms/batch 581.99 | loss  0.48 | cls  0.48 | err  0.16 | \n",
      "scGPT - INFO - | epoch  10 | 1200/1900 batches | lr 0.0000 | ms/batch 580.57 | loss  0.50 | cls  0.50 | err  0.16 | \n",
      "scGPT - INFO - | epoch  10 | 1300/1900 batches | lr 0.0000 | ms/batch 580.77 | loss  0.46 | cls  0.46 | err  0.15 | \n",
      "scGPT - INFO - | epoch  10 | 1400/1900 batches | lr 0.0000 | ms/batch 580.55 | loss  0.49 | cls  0.49 | err  0.18 | \n",
      "scGPT - INFO - | epoch  10 | 1500/1900 batches | lr 0.0000 | ms/batch 580.79 | loss  0.50 | cls  0.50 | err  0.16 | \n",
      "scGPT - INFO - | epoch  10 | 1600/1900 batches | lr 0.0000 | ms/batch 580.71 | loss  0.52 | cls  0.52 | err  0.17 | \n",
      "scGPT - INFO - | epoch  10 | 1700/1900 batches | lr 0.0000 | ms/batch 580.86 | loss  0.49 | cls  0.49 | err  0.17 | \n",
      "scGPT - INFO - | epoch  10 | 1800/1900 batches | lr 0.0000 | ms/batch 581.91 | loss  0.46 | cls  0.46 | err  0.15 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  10 | time: 1153.48s | valid loss/mse 0.5143 | err 0.1692\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.5143\n",
      "random masking at epoch  11, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  11 | 100/1900 batches | lr 0.0000 | ms/batch 593.95 | loss  0.48 | cls  0.48 | err  0.16 | \n",
      "scGPT - INFO - | epoch  11 | 200/1900 batches | lr 0.0000 | ms/batch 580.35 | loss  0.50 | cls  0.50 | err  0.16 | \n",
      "scGPT - INFO - | epoch  11 | 300/1900 batches | lr 0.0000 | ms/batch 580.42 | loss  0.46 | cls  0.46 | err  0.15 | \n",
      "scGPT - INFO - | epoch  11 | 400/1900 batches | lr 0.0000 | ms/batch 580.47 | loss  0.45 | cls  0.45 | err  0.15 | \n",
      "scGPT - INFO - | epoch  11 | 500/1900 batches | lr 0.0000 | ms/batch 580.42 | loss  0.46 | cls  0.46 | err  0.15 | \n",
      "scGPT - INFO - | epoch  11 | 600/1900 batches | lr 0.0000 | ms/batch 580.59 | loss  0.48 | cls  0.48 | err  0.16 | \n",
      "scGPT - INFO - | epoch  11 | 700/1900 batches | lr 0.0000 | ms/batch 582.35 | loss  0.47 | cls  0.47 | err  0.15 | \n",
      "scGPT - INFO - | epoch  11 | 800/1900 batches | lr 0.0000 | ms/batch 580.67 | loss  0.51 | cls  0.51 | err  0.16 | \n",
      "scGPT - INFO - | epoch  11 | 900/1900 batches | lr 0.0000 | ms/batch 580.63 | loss  0.49 | cls  0.49 | err  0.16 | \n",
      "scGPT - INFO - | epoch  11 | 1000/1900 batches | lr 0.0000 | ms/batch 581.47 | loss  0.44 | cls  0.44 | err  0.15 | \n",
      "scGPT - INFO - | epoch  11 | 1100/1900 batches | lr 0.0000 | ms/batch 580.45 | loss  0.47 | cls  0.47 | err  0.16 | \n",
      "scGPT - INFO - | epoch  11 | 1200/1900 batches | lr 0.0000 | ms/batch 580.38 | loss  0.48 | cls  0.48 | err  0.16 | \n",
      "scGPT - INFO - | epoch  11 | 1300/1900 batches | lr 0.0000 | ms/batch 582.02 | loss  0.44 | cls  0.44 | err  0.15 | \n",
      "scGPT - INFO - | epoch  11 | 1400/1900 batches | lr 0.0000 | ms/batch 580.38 | loss  0.47 | cls  0.47 | err  0.16 | \n",
      "scGPT - INFO - | epoch  11 | 1500/1900 batches | lr 0.0000 | ms/batch 580.83 | loss  0.47 | cls  0.47 | err  0.15 | \n",
      "scGPT - INFO - | epoch  11 | 1600/1900 batches | lr 0.0000 | ms/batch 580.57 | loss  0.51 | cls  0.51 | err  0.17 | \n",
      "scGPT - INFO - | epoch  11 | 1700/1900 batches | lr 0.0000 | ms/batch 580.66 | loss  0.47 | cls  0.47 | err  0.15 | \n",
      "scGPT - INFO - | epoch  11 | 1800/1900 batches | lr 0.0000 | ms/batch 580.53 | loss  0.45 | cls  0.45 | err  0.15 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  11 | time: 1152.99s | valid loss/mse 0.5276 | err 0.1716\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  12, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  12 | 100/1900 batches | lr 0.0000 | ms/batch 593.56 | loss  0.46 | cls  0.46 | err  0.16 | \n",
      "scGPT - INFO - | epoch  12 | 200/1900 batches | lr 0.0000 | ms/batch 580.86 | loss  0.48 | cls  0.48 | err  0.16 | \n",
      "scGPT - INFO - | epoch  12 | 300/1900 batches | lr 0.0000 | ms/batch 582.81 | loss  0.44 | cls  0.44 | err  0.14 | \n",
      "scGPT - INFO - | epoch  12 | 400/1900 batches | lr 0.0000 | ms/batch 580.99 | loss  0.45 | cls  0.45 | err  0.14 | \n",
      "scGPT - INFO - | epoch  12 | 500/1900 batches | lr 0.0000 | ms/batch 580.80 | loss  0.44 | cls  0.44 | err  0.14 | \n",
      "scGPT - INFO - | epoch  12 | 600/1900 batches | lr 0.0000 | ms/batch 581.02 | loss  0.47 | cls  0.47 | err  0.15 | \n",
      "scGPT - INFO - | epoch  12 | 700/1900 batches | lr 0.0000 | ms/batch 580.99 | loss  0.46 | cls  0.46 | err  0.15 | \n",
      "scGPT - INFO - | epoch  12 | 800/1900 batches | lr 0.0000 | ms/batch 581.00 | loss  0.50 | cls  0.50 | err  0.16 | \n",
      "scGPT - INFO - | epoch  12 | 900/1900 batches | lr 0.0000 | ms/batch 582.30 | loss  0.48 | cls  0.48 | err  0.15 | \n",
      "scGPT - INFO - | epoch  12 | 1000/1900 batches | lr 0.0000 | ms/batch 580.92 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  12 | 1100/1900 batches | lr 0.0000 | ms/batch 581.85 | loss  0.47 | cls  0.47 | err  0.16 | \n",
      "scGPT - INFO - | epoch  12 | 1200/1900 batches | lr 0.0000 | ms/batch 580.81 | loss  0.47 | cls  0.47 | err  0.16 | \n",
      "scGPT - INFO - | epoch  12 | 1300/1900 batches | lr 0.0000 | ms/batch 580.96 | loss  0.43 | cls  0.43 | err  0.15 | \n",
      "scGPT - INFO - | epoch  12 | 1400/1900 batches | lr 0.0000 | ms/batch 580.82 | loss  0.46 | cls  0.46 | err  0.16 | \n",
      "scGPT - INFO - | epoch  12 | 1500/1900 batches | lr 0.0000 | ms/batch 582.54 | loss  0.47 | cls  0.47 | err  0.15 | \n",
      "scGPT - INFO - | epoch  12 | 1600/1900 batches | lr 0.0000 | ms/batch 581.14 | loss  0.50 | cls  0.50 | err  0.17 | \n",
      "scGPT - INFO - | epoch  12 | 1700/1900 batches | lr 0.0000 | ms/batch 580.99 | loss  0.47 | cls  0.47 | err  0.15 | \n",
      "scGPT - INFO - | epoch  12 | 1800/1900 batches | lr 0.0000 | ms/batch 580.87 | loss  0.45 | cls  0.45 | err  0.15 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  12 | time: 1153.96s | valid loss/mse 0.5155 | err 0.1695\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  13, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  13 | 100/1900 batches | lr 0.0000 | ms/batch 592.49 | loss  0.46 | cls  0.46 | err  0.15 | \n",
      "scGPT - INFO - | epoch  13 | 200/1900 batches | lr 0.0000 | ms/batch 581.50 | loss  0.47 | cls  0.47 | err  0.16 | \n",
      "scGPT - INFO - | epoch  13 | 300/1900 batches | lr 0.0000 | ms/batch 580.93 | loss  0.42 | cls  0.42 | err  0.14 | \n",
      "scGPT - INFO - | epoch  13 | 400/1900 batches | lr 0.0000 | ms/batch 580.60 | loss  0.44 | cls  0.44 | err  0.14 | \n",
      "scGPT - INFO - | epoch  13 | 500/1900 batches | lr 0.0000 | ms/batch 582.44 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  13 | 600/1900 batches | lr 0.0000 | ms/batch 580.70 | loss  0.46 | cls  0.46 | err  0.14 | \n",
      "scGPT - INFO - | epoch  13 | 700/1900 batches | lr 0.0000 | ms/batch 580.71 | loss  0.45 | cls  0.45 | err  0.15 | \n",
      "scGPT - INFO - | epoch  13 | 800/1900 batches | lr 0.0000 | ms/batch 580.70 | loss  0.49 | cls  0.49 | err  0.15 | \n",
      "scGPT - INFO - | epoch  13 | 900/1900 batches | lr 0.0000 | ms/batch 580.65 | loss  0.47 | cls  0.47 | err  0.15 | \n",
      "scGPT - INFO - | epoch  13 | 1000/1900 batches | lr 0.0000 | ms/batch 580.57 | loss  0.43 | cls  0.43 | err  0.15 | \n",
      "scGPT - INFO - | epoch  13 | 1100/1900 batches | lr 0.0000 | ms/batch 581.82 | loss  0.45 | cls  0.45 | err  0.15 | \n",
      "scGPT - INFO - | epoch  13 | 1200/1900 batches | lr 0.0000 | ms/batch 581.33 | loss  0.47 | cls  0.47 | err  0.15 | \n",
      "scGPT - INFO - | epoch  13 | 1300/1900 batches | lr 0.0000 | ms/batch 580.67 | loss  0.42 | cls  0.42 | err  0.14 | \n",
      "scGPT - INFO - | epoch  13 | 1400/1900 batches | lr 0.0000 | ms/batch 580.37 | loss  0.46 | cls  0.46 | err  0.15 | \n",
      "scGPT - INFO - | epoch  13 | 1500/1900 batches | lr 0.0000 | ms/batch 580.74 | loss  0.46 | cls  0.46 | err  0.14 | \n",
      "scGPT - INFO - | epoch  13 | 1600/1900 batches | lr 0.0000 | ms/batch 580.75 | loss  0.49 | cls  0.49 | err  0.17 | \n",
      "scGPT - INFO - | epoch  13 | 1700/1900 batches | lr 0.0000 | ms/batch 582.06 | loss  0.46 | cls  0.46 | err  0.15 | \n",
      "scGPT - INFO - | epoch  13 | 1800/1900 batches | lr 0.0000 | ms/batch 580.20 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  13 | time: 1152.97s | valid loss/mse 0.5064 | err 0.1682\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.5064\n",
      "random masking at epoch  14, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  14 | 100/1900 batches | lr 0.0000 | ms/batch 594.63 | loss  0.45 | cls  0.45 | err  0.14 | \n",
      "scGPT - INFO - | epoch  14 | 200/1900 batches | lr 0.0000 | ms/batch 580.28 | loss  0.46 | cls  0.46 | err  0.15 | \n",
      "scGPT - INFO - | epoch  14 | 300/1900 batches | lr 0.0000 | ms/batch 581.35 | loss  0.42 | cls  0.42 | err  0.13 | \n",
      "scGPT - INFO - | epoch  14 | 400/1900 batches | lr 0.0000 | ms/batch 581.71 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  14 | 500/1900 batches | lr 0.0000 | ms/batch 580.51 | loss  0.44 | cls  0.44 | err  0.14 | \n",
      "scGPT - INFO - | epoch  14 | 600/1900 batches | lr 0.0000 | ms/batch 580.71 | loss  0.46 | cls  0.46 | err  0.15 | \n",
      "scGPT - INFO - | epoch  14 | 700/1900 batches | lr 0.0000 | ms/batch 582.18 | loss  0.44 | cls  0.44 | err  0.15 | \n",
      "scGPT - INFO - | epoch  14 | 800/1900 batches | lr 0.0000 | ms/batch 580.71 | loss  0.48 | cls  0.48 | err  0.15 | \n",
      "scGPT - INFO - | epoch  14 | 900/1900 batches | lr 0.0000 | ms/batch 580.95 | loss  0.46 | cls  0.46 | err  0.15 | \n",
      "scGPT - INFO - | epoch  14 | 1000/1900 batches | lr 0.0000 | ms/batch 580.82 | loss  0.42 | cls  0.42 | err  0.15 | \n",
      "scGPT - INFO - | epoch  14 | 1100/1900 batches | lr 0.0000 | ms/batch 582.09 | loss  0.44 | cls  0.44 | err  0.15 | \n",
      "scGPT - INFO - | epoch  14 | 1200/1900 batches | lr 0.0000 | ms/batch 580.65 | loss  0.46 | cls  0.46 | err  0.16 | \n",
      "scGPT - INFO - | epoch  14 | 1300/1900 batches | lr 0.0000 | ms/batch 581.58 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  14 | 1400/1900 batches | lr 0.0000 | ms/batch 582.12 | loss  0.44 | cls  0.44 | err  0.15 | \n",
      "scGPT - INFO - | epoch  14 | 1500/1900 batches | lr 0.0000 | ms/batch 580.84 | loss  0.44 | cls  0.44 | err  0.15 | \n",
      "scGPT - INFO - | epoch  14 | 1600/1900 batches | lr 0.0000 | ms/batch 580.84 | loss  0.48 | cls  0.48 | err  0.17 | \n",
      "scGPT - INFO - | epoch  14 | 1700/1900 batches | lr 0.0000 | ms/batch 580.76 | loss  0.45 | cls  0.45 | err  0.15 | \n",
      "scGPT - INFO - | epoch  14 | 1800/1900 batches | lr 0.0000 | ms/batch 581.82 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  14 | time: 1153.86s | valid loss/mse 0.5084 | err 0.1689\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  15, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  15 | 100/1900 batches | lr 0.0000 | ms/batch 592.08 | loss  0.44 | cls  0.44 | err  0.15 | \n",
      "scGPT - INFO - | epoch  15 | 200/1900 batches | lr 0.0000 | ms/batch 579.99 | loss  0.46 | cls  0.46 | err  0.15 | \n",
      "scGPT - INFO - | epoch  15 | 300/1900 batches | lr 0.0000 | ms/batch 582.17 | loss  0.41 | cls  0.41 | err  0.13 | \n",
      "scGPT - INFO - | epoch  15 | 400/1900 batches | lr 0.0000 | ms/batch 581.18 | loss  0.42 | cls  0.42 | err  0.13 | \n",
      "scGPT - INFO - | epoch  15 | 500/1900 batches | lr 0.0000 | ms/batch 581.77 | loss  0.42 | cls  0.42 | err  0.13 | \n",
      "scGPT - INFO - | epoch  15 | 600/1900 batches | lr 0.0000 | ms/batch 580.32 | loss  0.45 | cls  0.45 | err  0.14 | \n",
      "scGPT - INFO - | epoch  15 | 700/1900 batches | lr 0.0000 | ms/batch 581.96 | loss  0.43 | cls  0.43 | err  0.15 | \n",
      "scGPT - INFO - | epoch  15 | 800/1900 batches | lr 0.0000 | ms/batch 580.32 | loss  0.47 | cls  0.47 | err  0.15 | \n",
      "scGPT - INFO - | epoch  15 | 900/1900 batches | lr 0.0000 | ms/batch 580.41 | loss  0.47 | cls  0.47 | err  0.15 | \n",
      "scGPT - INFO - | epoch  15 | 1000/1900 batches | lr 0.0000 | ms/batch 581.77 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  15 | 1100/1900 batches | lr 0.0000 | ms/batch 580.29 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  15 | 1200/1900 batches | lr 0.0000 | ms/batch 581.82 | loss  0.45 | cls  0.45 | err  0.15 | \n",
      "scGPT - INFO - | epoch  15 | 1300/1900 batches | lr 0.0000 | ms/batch 580.34 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  15 | 1400/1900 batches | lr 0.0000 | ms/batch 581.29 | loss  0.44 | cls  0.44 | err  0.15 | \n",
      "scGPT - INFO - | epoch  15 | 1500/1900 batches | lr 0.0000 | ms/batch 581.85 | loss  0.45 | cls  0.45 | err  0.15 | \n",
      "scGPT - INFO - | epoch  15 | 1600/1900 batches | lr 0.0000 | ms/batch 580.38 | loss  0.47 | cls  0.47 | err  0.16 | \n",
      "scGPT - INFO - | epoch  15 | 1700/1900 batches | lr 0.0000 | ms/batch 581.75 | loss  0.44 | cls  0.44 | err  0.14 | \n",
      "scGPT - INFO - | epoch  15 | 1800/1900 batches | lr 0.0000 | ms/batch 580.43 | loss  0.41 | cls  0.41 | err  0.13 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  15 | time: 1153.23s | valid loss/mse 0.4890 | err 0.1618\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.4890\n",
      "random masking at epoch  16, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  16 | 100/1900 batches | lr 0.0000 | ms/batch 594.40 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  16 | 200/1900 batches | lr 0.0000 | ms/batch 580.19 | loss  0.46 | cls  0.46 | err  0.15 | \n",
      "scGPT - INFO - | epoch  16 | 300/1900 batches | lr 0.0000 | ms/batch 580.43 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  16 | 400/1900 batches | lr 0.0000 | ms/batch 581.77 | loss  0.42 | cls  0.42 | err  0.13 | \n",
      "scGPT - INFO - | epoch  16 | 500/1900 batches | lr 0.0000 | ms/batch 581.19 | loss  0.42 | cls  0.42 | err  0.14 | \n",
      "scGPT - INFO - | epoch  16 | 600/1900 batches | lr 0.0000 | ms/batch 581.61 | loss  0.45 | cls  0.45 | err  0.14 | \n",
      "scGPT - INFO - | epoch  16 | 700/1900 batches | lr 0.0000 | ms/batch 580.20 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  16 | 800/1900 batches | lr 0.0000 | ms/batch 580.17 | loss  0.47 | cls  0.47 | err  0.15 | \n",
      "scGPT - INFO - | epoch  16 | 900/1900 batches | lr 0.0000 | ms/batch 580.22 | loss  0.45 | cls  0.45 | err  0.15 | \n",
      "scGPT - INFO - | epoch  16 | 1000/1900 batches | lr 0.0000 | ms/batch 580.30 | loss  0.42 | cls  0.42 | err  0.14 | \n",
      "scGPT - INFO - | epoch  16 | 1100/1900 batches | lr 0.0000 | ms/batch 580.17 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  16 | 1200/1900 batches | lr 0.0000 | ms/batch 581.61 | loss  0.45 | cls  0.45 | err  0.14 | \n",
      "scGPT - INFO - | epoch  16 | 1300/1900 batches | lr 0.0000 | ms/batch 580.06 | loss  0.40 | cls  0.40 | err  0.14 | \n",
      "scGPT - INFO - | epoch  16 | 1400/1900 batches | lr 0.0000 | ms/batch 580.11 | loss  0.44 | cls  0.44 | err  0.15 | \n",
      "scGPT - INFO - | epoch  16 | 1500/1900 batches | lr 0.0000 | ms/batch 581.10 | loss  0.44 | cls  0.44 | err  0.14 | \n",
      "scGPT - INFO - | epoch  16 | 1600/1900 batches | lr 0.0000 | ms/batch 580.36 | loss  0.47 | cls  0.47 | err  0.16 | \n",
      "scGPT - INFO - | epoch  16 | 1700/1900 batches | lr 0.0000 | ms/batch 580.26 | loss  0.44 | cls  0.44 | err  0.14 | \n",
      "scGPT - INFO - | epoch  16 | 1800/1900 batches | lr 0.0000 | ms/batch 581.48 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  16 | time: 1152.74s | valid loss/mse 0.4842 | err 0.1613\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.4842\n",
      "random masking at epoch  17, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  17 | 100/1900 batches | lr 0.0000 | ms/batch 594.75 | loss  0.42 | cls  0.42 | err  0.14 | \n",
      "scGPT - INFO - | epoch  17 | 200/1900 batches | lr 0.0000 | ms/batch 580.64 | loss  0.44 | cls  0.44 | err  0.14 | \n",
      "scGPT - INFO - | epoch  17 | 300/1900 batches | lr 0.0000 | ms/batch 580.62 | loss  0.40 | cls  0.40 | err  0.14 | \n",
      "scGPT - INFO - | epoch  17 | 400/1900 batches | lr 0.0000 | ms/batch 580.53 | loss  0.41 | cls  0.41 | err  0.13 | \n",
      "scGPT - INFO - | epoch  17 | 500/1900 batches | lr 0.0000 | ms/batch 581.77 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  17 | 600/1900 batches | lr 0.0000 | ms/batch 581.49 | loss  0.44 | cls  0.44 | err  0.14 | \n",
      "scGPT - INFO - | epoch  17 | 700/1900 batches | lr 0.0000 | ms/batch 580.47 | loss  0.42 | cls  0.42 | err  0.15 | \n",
      "scGPT - INFO - | epoch  17 | 800/1900 batches | lr 0.0000 | ms/batch 581.84 | loss  0.46 | cls  0.46 | err  0.14 | \n",
      "scGPT - INFO - | epoch  17 | 900/1900 batches | lr 0.0000 | ms/batch 580.44 | loss  0.44 | cls  0.44 | err  0.15 | \n",
      "scGPT - INFO - | epoch  17 | 1000/1900 batches | lr 0.0000 | ms/batch 580.32 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - | epoch  17 | 1100/1900 batches | lr 0.0000 | ms/batch 580.36 | loss  0.43 | cls  0.43 | err  0.13 | \n",
      "scGPT - INFO - | epoch  17 | 1200/1900 batches | lr 0.0000 | ms/batch 581.64 | loss  0.44 | cls  0.44 | err  0.15 | \n",
      "scGPT - INFO - | epoch  17 | 1300/1900 batches | lr 0.0000 | ms/batch 580.32 | loss  0.39 | cls  0.39 | err  0.13 | \n",
      "scGPT - INFO - | epoch  17 | 1400/1900 batches | lr 0.0000 | ms/batch 580.31 | loss  0.43 | cls  0.43 | err  0.15 | \n",
      "scGPT - INFO - | epoch  17 | 1500/1900 batches | lr 0.0000 | ms/batch 581.78 | loss  0.43 | cls  0.43 | err  0.15 | \n",
      "scGPT - INFO - | epoch  17 | 1600/1900 batches | lr 0.0000 | ms/batch 581.22 | loss  0.46 | cls  0.46 | err  0.16 | \n",
      "scGPT - INFO - | epoch  17 | 1700/1900 batches | lr 0.0000 | ms/batch 580.32 | loss  0.44 | cls  0.44 | err  0.14 | \n",
      "scGPT - INFO - | epoch  17 | 1800/1900 batches | lr 0.0000 | ms/batch 581.58 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  17 | time: 1153.18s | valid loss/mse 0.4734 | err 0.1605\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.4734\n",
      "random masking at epoch  18, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  18 | 100/1900 batches | lr 0.0000 | ms/batch 594.73 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  18 | 200/1900 batches | lr 0.0000 | ms/batch 580.70 | loss  0.44 | cls  0.44 | err  0.15 | \n",
      "scGPT - INFO - | epoch  18 | 300/1900 batches | lr 0.0000 | ms/batch 580.84 | loss  0.39 | cls  0.39 | err  0.13 | \n",
      "scGPT - INFO - | epoch  18 | 400/1900 batches | lr 0.0000 | ms/batch 582.12 | loss  0.41 | cls  0.41 | err  0.13 | \n",
      "scGPT - INFO - | epoch  18 | 500/1900 batches | lr 0.0000 | ms/batch 580.68 | loss  0.41 | cls  0.41 | err  0.13 | \n",
      "scGPT - INFO - | epoch  18 | 600/1900 batches | lr 0.0000 | ms/batch 580.92 | loss  0.44 | cls  0.44 | err  0.14 | \n",
      "scGPT - INFO - | epoch  18 | 700/1900 batches | lr 0.0000 | ms/batch 581.84 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  18 | 800/1900 batches | lr 0.0000 | ms/batch 582.27 | loss  0.45 | cls  0.45 | err  0.14 | \n",
      "scGPT - INFO - | epoch  18 | 900/1900 batches | lr 0.0000 | ms/batch 580.94 | loss  0.44 | cls  0.44 | err  0.14 | \n",
      "scGPT - INFO - | epoch  18 | 1000/1900 batches | lr 0.0000 | ms/batch 580.88 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - | epoch  18 | 1100/1900 batches | lr 0.0000 | ms/batch 582.20 | loss  0.41 | cls  0.41 | err  0.13 | \n",
      "scGPT - INFO - | epoch  18 | 1200/1900 batches | lr 0.0000 | ms/batch 580.57 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  18 | 1300/1900 batches | lr 0.0000 | ms/batch 580.66 | loss  0.39 | cls  0.39 | err  0.14 | \n",
      "scGPT - INFO - | epoch  18 | 1400/1900 batches | lr 0.0000 | ms/batch 582.11 | loss  0.43 | cls  0.43 | err  0.15 | \n",
      "scGPT - INFO - | epoch  18 | 1500/1900 batches | lr 0.0000 | ms/batch 580.62 | loss  0.42 | cls  0.42 | err  0.14 | \n",
      "scGPT - INFO - | epoch  18 | 1600/1900 batches | lr 0.0000 | ms/batch 580.78 | loss  0.45 | cls  0.45 | err  0.16 | \n",
      "scGPT - INFO - | epoch  18 | 1700/1900 batches | lr 0.0000 | ms/batch 581.75 | loss  0.42 | cls  0.42 | err  0.15 | \n",
      "scGPT - INFO - | epoch  18 | 1800/1900 batches | lr 0.0000 | ms/batch 582.01 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  18 | time: 1154.01s | valid loss/mse 0.4669 | err 0.1589\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.4669\n",
      "random masking at epoch  19, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  19 | 100/1900 batches | lr 0.0000 | ms/batch 592.44 | loss  0.41 | cls  0.41 | err  0.13 | \n",
      "scGPT - INFO - | epoch  19 | 200/1900 batches | lr 0.0000 | ms/batch 580.68 | loss  0.45 | cls  0.45 | err  0.15 | \n",
      "scGPT - INFO - | epoch  19 | 300/1900 batches | lr 0.0000 | ms/batch 580.85 | loss  0.39 | cls  0.39 | err  0.13 | \n",
      "scGPT - INFO - | epoch  19 | 400/1900 batches | lr 0.0000 | ms/batch 580.60 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - | epoch  19 | 500/1900 batches | lr 0.0000 | ms/batch 580.61 | loss  0.40 | cls  0.40 | err  0.14 | \n",
      "scGPT - INFO - | epoch  19 | 600/1900 batches | lr 0.0000 | ms/batch 582.76 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  19 | 700/1900 batches | lr 0.0000 | ms/batch 580.89 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  19 | 800/1900 batches | lr 0.0000 | ms/batch 581.77 | loss  0.45 | cls  0.45 | err  0.15 | \n",
      "scGPT - INFO - | epoch  19 | 900/1900 batches | lr 0.0000 | ms/batch 580.71 | loss  0.44 | cls  0.44 | err  0.15 | \n",
      "scGPT - INFO - | epoch  19 | 1000/1900 batches | lr 0.0000 | ms/batch 580.42 | loss  0.39 | cls  0.39 | err  0.14 | \n",
      "scGPT - INFO - | epoch  19 | 1100/1900 batches | lr 0.0000 | ms/batch 580.42 | loss  0.41 | cls  0.41 | err  0.13 | \n",
      "scGPT - INFO - | epoch  19 | 1200/1900 batches | lr 0.0000 | ms/batch 581.82 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  19 | 1300/1900 batches | lr 0.0000 | ms/batch 580.63 | loss  0.39 | cls  0.39 | err  0.13 | \n",
      "scGPT - INFO - | epoch  19 | 1400/1900 batches | lr 0.0000 | ms/batch 580.46 | loss  0.42 | cls  0.42 | err  0.14 | \n",
      "scGPT - INFO - | epoch  19 | 1500/1900 batches | lr 0.0000 | ms/batch 580.67 | loss  0.42 | cls  0.42 | err  0.14 | \n",
      "scGPT - INFO - | epoch  19 | 1600/1900 batches | lr 0.0000 | ms/batch 580.54 | loss  0.45 | cls  0.45 | err  0.16 | \n",
      "scGPT - INFO - | epoch  19 | 1700/1900 batches | lr 0.0000 | ms/batch 580.53 | loss  0.42 | cls  0.42 | err  0.15 | \n",
      "scGPT - INFO - | epoch  19 | 1800/1900 batches | lr 0.0000 | ms/batch 581.30 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  19 | time: 1153.13s | valid loss/mse 0.4630 | err 0.1534\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.4630\n",
      "random masking at epoch  20, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  20 | 100/1900 batches | lr 0.0000 | ms/batch 592.43 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  20 | 200/1900 batches | lr 0.0000 | ms/batch 582.31 | loss  0.44 | cls  0.44 | err  0.15 | \n",
      "scGPT - INFO - | epoch  20 | 300/1900 batches | lr 0.0000 | ms/batch 580.27 | loss  0.39 | cls  0.39 | err  0.13 | \n",
      "scGPT - INFO - | epoch  20 | 400/1900 batches | lr 0.0000 | ms/batch 580.38 | loss  0.39 | cls  0.39 | err  0.13 | \n",
      "scGPT - INFO - | epoch  20 | 500/1900 batches | lr 0.0000 | ms/batch 580.13 | loss  0.40 | cls  0.40 | err  0.14 | \n",
      "scGPT - INFO - | epoch  20 | 600/1900 batches | lr 0.0000 | ms/batch 580.10 | loss  0.44 | cls  0.44 | err  0.15 | \n",
      "scGPT - INFO - | epoch  20 | 700/1900 batches | lr 0.0000 | ms/batch 580.35 | loss  0.42 | cls  0.42 | err  0.15 | \n",
      "scGPT - INFO - | epoch  20 | 800/1900 batches | lr 0.0000 | ms/batch 581.80 | loss  0.45 | cls  0.45 | err  0.15 | \n",
      "scGPT - INFO - | epoch  20 | 900/1900 batches | lr 0.0000 | ms/batch 581.22 | loss  0.43 | cls  0.43 | err  0.15 | \n",
      "scGPT - INFO - | epoch  20 | 1000/1900 batches | lr 0.0000 | ms/batch 580.22 | loss  0.39 | cls  0.39 | err  0.14 | \n",
      "scGPT - INFO - | epoch  20 | 1100/1900 batches | lr 0.0000 | ms/batch 580.33 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  20 | 1200/1900 batches | lr 0.0000 | ms/batch 580.32 | loss  0.42 | cls  0.42 | err  0.14 | \n",
      "scGPT - INFO - | epoch  20 | 1300/1900 batches | lr 0.0000 | ms/batch 580.47 | loss  0.38 | cls  0.38 | err  0.13 | \n",
      "scGPT - INFO - | epoch  20 | 1400/1900 batches | lr 0.0000 | ms/batch 581.56 | loss  0.42 | cls  0.42 | err  0.14 | \n",
      "scGPT - INFO - | epoch  20 | 1500/1900 batches | lr 0.0000 | ms/batch 580.18 | loss  0.42 | cls  0.42 | err  0.14 | \n",
      "scGPT - INFO - | epoch  20 | 1600/1900 batches | lr 0.0000 | ms/batch 580.21 | loss  0.45 | cls  0.45 | err  0.15 | \n",
      "scGPT - INFO - | epoch  20 | 1700/1900 batches | lr 0.0000 | ms/batch 580.20 | loss  0.43 | cls  0.43 | err  0.15 | \n",
      "scGPT - INFO - | epoch  20 | 1800/1900 batches | lr 0.0000 | ms/batch 580.15 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  20 | time: 1152.67s | valid loss/mse 0.4694 | err 0.1587\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  21, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  21 | 100/1900 batches | lr 0.0000 | ms/batch 592.12 | loss  0.41 | cls  0.41 | err  0.13 | \n",
      "scGPT - INFO - | epoch  21 | 200/1900 batches | lr 0.0000 | ms/batch 580.19 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  21 | 300/1900 batches | lr 0.0000 | ms/batch 580.24 | loss  0.38 | cls  0.38 | err  0.12 | \n",
      "scGPT - INFO - | epoch  21 | 400/1900 batches | lr 0.0000 | ms/batch 582.03 | loss  0.39 | cls  0.39 | err  0.12 | \n",
      "scGPT - INFO - | epoch  21 | 500/1900 batches | lr 0.0000 | ms/batch 580.08 | loss  0.39 | cls  0.39 | err  0.13 | \n",
      "scGPT - INFO - | epoch  21 | 600/1900 batches | lr 0.0000 | ms/batch 580.41 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  21 | 700/1900 batches | lr 0.0000 | ms/batch 580.40 | loss  0.41 | cls  0.41 | err  0.13 | \n",
      "scGPT - INFO - | epoch  21 | 800/1900 batches | lr 0.0000 | ms/batch 580.29 | loss  0.44 | cls  0.44 | err  0.14 | \n",
      "scGPT - INFO - | epoch  21 | 900/1900 batches | lr 0.0000 | ms/batch 580.50 | loss  0.43 | cls  0.43 | err  0.15 | \n",
      "scGPT - INFO - | epoch  21 | 1000/1900 batches | lr 0.0000 | ms/batch 582.40 | loss  0.38 | cls  0.38 | err  0.13 | \n",
      "scGPT - INFO - | epoch  21 | 1100/1900 batches | lr 0.0000 | ms/batch 579.93 | loss  0.41 | cls  0.41 | err  0.13 | \n",
      "scGPT - INFO - | epoch  21 | 1200/1900 batches | lr 0.0000 | ms/batch 580.01 | loss  0.42 | cls  0.42 | err  0.14 | \n",
      "scGPT - INFO - | epoch  21 | 1300/1900 batches | lr 0.0000 | ms/batch 580.23 | loss  0.38 | cls  0.38 | err  0.13 | \n",
      "scGPT - INFO - | epoch  21 | 1400/1900 batches | lr 0.0000 | ms/batch 580.12 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  21 | 1500/1900 batches | lr 0.0000 | ms/batch 580.16 | loss  0.42 | cls  0.42 | err  0.14 | \n",
      "scGPT - INFO - | epoch  21 | 1600/1900 batches | lr 0.0000 | ms/batch 581.54 | loss  0.45 | cls  0.45 | err  0.15 | \n",
      "scGPT - INFO - | epoch  21 | 1700/1900 batches | lr 0.0000 | ms/batch 580.06 | loss  0.42 | cls  0.42 | err  0.15 | \n",
      "scGPT - INFO - | epoch  21 | 1800/1900 batches | lr 0.0000 | ms/batch 579.91 | loss  0.39 | cls  0.39 | err  0.13 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  21 | time: 1152.46s | valid loss/mse 0.4609 | err 0.1529\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.4609\n",
      "random masking at epoch  22, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  22 | 100/1900 batches | lr 0.0000 | ms/batch 592.97 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - | epoch  22 | 200/1900 batches | lr 0.0000 | ms/batch 579.55 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  22 | 300/1900 batches | lr 0.0000 | ms/batch 581.77 | loss  0.39 | cls  0.39 | err  0.12 | \n",
      "scGPT - INFO - | epoch  22 | 400/1900 batches | lr 0.0000 | ms/batch 579.65 | loss  0.38 | cls  0.38 | err  0.12 | \n",
      "scGPT - INFO - | epoch  22 | 500/1900 batches | lr 0.0000 | ms/batch 579.76 | loss  0.39 | cls  0.39 | err  0.13 | \n",
      "scGPT - INFO - | epoch  22 | 600/1900 batches | lr 0.0000 | ms/batch 579.98 | loss  0.43 | cls  0.43 | err  0.15 | \n",
      "scGPT - INFO - | epoch  22 | 700/1900 batches | lr 0.0000 | ms/batch 581.36 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - | epoch  22 | 800/1900 batches | lr 0.0000 | ms/batch 580.02 | loss  0.44 | cls  0.44 | err  0.14 | \n",
      "scGPT - INFO - | epoch  22 | 900/1900 batches | lr 0.0000 | ms/batch 579.99 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  22 | 1000/1900 batches | lr 0.0000 | ms/batch 581.29 | loss  0.38 | cls  0.38 | err  0.14 | \n",
      "scGPT - INFO - | epoch  22 | 1100/1900 batches | lr 0.0000 | ms/batch 580.82 | loss  0.41 | cls  0.41 | err  0.13 | \n",
      "scGPT - INFO - | epoch  22 | 1200/1900 batches | lr 0.0000 | ms/batch 579.79 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  22 | 1300/1900 batches | lr 0.0000 | ms/batch 579.89 | loss  0.38 | cls  0.38 | err  0.13 | \n",
      "scGPT - INFO - | epoch  22 | 1400/1900 batches | lr 0.0000 | ms/batch 581.13 | loss  0.41 | cls  0.41 | err  0.15 | \n",
      "scGPT - INFO - | epoch  22 | 1500/1900 batches | lr 0.0000 | ms/batch 579.81 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  22 | 1600/1900 batches | lr 0.0000 | ms/batch 579.76 | loss  0.44 | cls  0.44 | err  0.16 | \n",
      "scGPT - INFO - | epoch  22 | 1700/1900 batches | lr 0.0000 | ms/batch 581.13 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  22 | 1800/1900 batches | lr 0.0000 | ms/batch 579.71 | loss  0.38 | cls  0.38 | err  0.12 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  22 | time: 1152.07s | valid loss/mse 0.4597 | err 0.1526\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.4597\n",
      "random masking at epoch  23, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  23 | 100/1900 batches | lr 0.0000 | ms/batch 594.00 | loss  0.40 | cls  0.40 | err  0.14 | \n",
      "scGPT - INFO - | epoch  23 | 200/1900 batches | lr 0.0000 | ms/batch 580.96 | loss  0.43 | cls  0.43 | err  0.15 | \n",
      "scGPT - INFO - | epoch  23 | 300/1900 batches | lr 0.0000 | ms/batch 580.41 | loss  0.38 | cls  0.38 | err  0.13 | \n",
      "scGPT - INFO - | epoch  23 | 400/1900 batches | lr 0.0000 | ms/batch 581.83 | loss  0.39 | cls  0.39 | err  0.13 | \n",
      "scGPT - INFO - | epoch  23 | 500/1900 batches | lr 0.0000 | ms/batch 580.38 | loss  0.39 | cls  0.39 | err  0.13 | \n",
      "scGPT - INFO - | epoch  23 | 600/1900 batches | lr 0.0000 | ms/batch 581.87 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  23 | 700/1900 batches | lr 0.0000 | ms/batch 580.72 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  23 | 800/1900 batches | lr 0.0000 | ms/batch 582.11 | loss  0.44 | cls  0.44 | err  0.14 | \n",
      "scGPT - INFO - | epoch  23 | 900/1900 batches | lr 0.0000 | ms/batch 580.75 | loss  0.42 | cls  0.42 | err  0.14 | \n",
      "scGPT - INFO - | epoch  23 | 1000/1900 batches | lr 0.0000 | ms/batch 580.59 | loss  0.37 | cls  0.37 | err  0.13 | \n",
      "scGPT - INFO - | epoch  23 | 1100/1900 batches | lr 0.0000 | ms/batch 581.93 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  23 | 1200/1900 batches | lr 0.0000 | ms/batch 581.49 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  23 | 1300/1900 batches | lr 0.0000 | ms/batch 582.08 | loss  0.37 | cls  0.37 | err  0.12 | \n",
      "scGPT - INFO - | epoch  23 | 1400/1900 batches | lr 0.0000 | ms/batch 580.55 | loss  0.41 | cls  0.41 | err  0.15 | \n",
      "scGPT - INFO - | epoch  23 | 1500/1900 batches | lr 0.0000 | ms/batch 580.74 | loss  0.41 | cls  0.41 | err  0.13 | \n",
      "scGPT - INFO - | epoch  23 | 1600/1900 batches | lr 0.0000 | ms/batch 582.24 | loss  0.44 | cls  0.44 | err  0.15 | \n",
      "scGPT - INFO - | epoch  23 | 1700/1900 batches | lr 0.0000 | ms/batch 580.72 | loss  0.40 | cls  0.40 | err  0.14 | \n",
      "scGPT - INFO - | epoch  23 | 1800/1900 batches | lr 0.0000 | ms/batch 582.16 | loss  0.38 | cls  0.38 | err  0.12 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  23 | time: 1153.95s | valid loss/mse 0.4557 | err 0.1526\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.4557\n",
      "random masking at epoch  24, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  24 | 100/1900 batches | lr 0.0000 | ms/batch 592.42 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - | epoch  24 | 200/1900 batches | lr 0.0000 | ms/batch 580.57 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  24 | 300/1900 batches | lr 0.0000 | ms/batch 581.19 | loss  0.38 | cls  0.38 | err  0.12 | \n",
      "scGPT - INFO - | epoch  24 | 400/1900 batches | lr 0.0000 | ms/batch 580.57 | loss  0.38 | cls  0.38 | err  0.13 | \n",
      "scGPT - INFO - | epoch  24 | 500/1900 batches | lr 0.0000 | ms/batch 582.36 | loss  0.38 | cls  0.38 | err  0.13 | \n",
      "scGPT - INFO - | epoch  24 | 600/1900 batches | lr 0.0000 | ms/batch 580.52 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  24 | 700/1900 batches | lr 0.0000 | ms/batch 580.58 | loss  0.39 | cls  0.39 | err  0.13 | \n",
      "scGPT - INFO - | epoch  24 | 800/1900 batches | lr 0.0000 | ms/batch 580.74 | loss  0.44 | cls  0.44 | err  0.14 | \n",
      "scGPT - INFO - | epoch  24 | 900/1900 batches | lr 0.0000 | ms/batch 580.75 | loss  0.42 | cls  0.42 | err  0.15 | \n",
      "scGPT - INFO - | epoch  24 | 1000/1900 batches | lr 0.0000 | ms/batch 580.58 | loss  0.37 | cls  0.37 | err  0.13 | \n",
      "scGPT - INFO - | epoch  24 | 1100/1900 batches | lr 0.0000 | ms/batch 581.84 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - | epoch  24 | 1200/1900 batches | lr 0.0000 | ms/batch 580.41 | loss  0.42 | cls  0.42 | err  0.13 | \n",
      "scGPT - INFO - | epoch  24 | 1300/1900 batches | lr 0.0000 | ms/batch 581.17 | loss  0.38 | cls  0.38 | err  0.12 | \n",
      "scGPT - INFO - | epoch  24 | 1400/1900 batches | lr 0.0000 | ms/batch 580.21 | loss  0.40 | cls  0.40 | err  0.14 | \n",
      "scGPT - INFO - | epoch  24 | 1500/1900 batches | lr 0.0000 | ms/batch 580.33 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - | epoch  24 | 1600/1900 batches | lr 0.0000 | ms/batch 580.41 | loss  0.44 | cls  0.44 | err  0.15 | \n",
      "scGPT - INFO - | epoch  24 | 1700/1900 batches | lr 0.0000 | ms/batch 580.36 | loss  0.40 | cls  0.40 | err  0.14 | \n",
      "scGPT - INFO - | epoch  24 | 1800/1900 batches | lr 0.0000 | ms/batch 581.62 | loss  0.38 | cls  0.38 | err  0.12 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  24 | time: 1152.79s | valid loss/mse 0.4513 | err 0.1505\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.4513\n",
      "random masking at epoch  25, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  25 | 100/1900 batches | lr 0.0000 | ms/batch 594.14 | loss  0.40 | cls  0.40 | err  0.14 | \n",
      "scGPT - INFO - | epoch  25 | 200/1900 batches | lr 0.0000 | ms/batch 580.23 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  25 | 300/1900 batches | lr 0.0000 | ms/batch 580.10 | loss  0.37 | cls  0.37 | err  0.12 | \n",
      "scGPT - INFO - | epoch  25 | 400/1900 batches | lr 0.0000 | ms/batch 581.12 | loss  0.38 | cls  0.38 | err  0.13 | \n",
      "scGPT - INFO - | epoch  25 | 500/1900 batches | lr 0.0000 | ms/batch 580.28 | loss  0.38 | cls  0.38 | err  0.13 | \n",
      "scGPT - INFO - | epoch  25 | 600/1900 batches | lr 0.0000 | ms/batch 580.37 | loss  0.42 | cls  0.42 | err  0.14 | \n",
      "scGPT - INFO - | epoch  25 | 700/1900 batches | lr 0.0000 | ms/batch 581.85 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - | epoch  25 | 800/1900 batches | lr 0.0000 | ms/batch 580.38 | loss  0.44 | cls  0.44 | err  0.14 | \n",
      "scGPT - INFO - | epoch  25 | 900/1900 batches | lr 0.0000 | ms/batch 580.36 | loss  0.42 | cls  0.42 | err  0.15 | \n",
      "scGPT - INFO - | epoch  25 | 1000/1900 batches | lr 0.0000 | ms/batch 580.27 | loss  0.37 | cls  0.37 | err  0.13 | \n",
      "scGPT - INFO - | epoch  25 | 1100/1900 batches | lr 0.0000 | ms/batch 580.12 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - | epoch  25 | 1200/1900 batches | lr 0.0000 | ms/batch 580.06 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  25 | 1300/1900 batches | lr 0.0000 | ms/batch 581.66 | loss  0.37 | cls  0.37 | err  0.13 | \n",
      "scGPT - INFO - | epoch  25 | 1400/1900 batches | lr 0.0000 | ms/batch 581.06 | loss  0.39 | cls  0.39 | err  0.14 | \n",
      "scGPT - INFO - | epoch  25 | 1500/1900 batches | lr 0.0000 | ms/batch 580.19 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  25 | 1600/1900 batches | lr 0.0000 | ms/batch 580.14 | loss  0.44 | cls  0.44 | err  0.15 | \n",
      "scGPT - INFO - | epoch  25 | 1700/1900 batches | lr 0.0000 | ms/batch 580.21 | loss  0.40 | cls  0.40 | err  0.14 | \n",
      "scGPT - INFO - | epoch  25 | 1800/1900 batches | lr 0.0000 | ms/batch 580.21 | loss  0.38 | cls  0.38 | err  0.12 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  25 | time: 1152.53s | valid loss/mse 0.4509 | err 0.1505\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.4509\n",
      "random masking at epoch  26, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  26 | 100/1900 batches | lr 0.0000 | ms/batch 592.57 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - | epoch  26 | 200/1900 batches | lr 0.0000 | ms/batch 580.39 | loss  0.42 | cls  0.42 | err  0.14 | \n",
      "scGPT - INFO - | epoch  26 | 300/1900 batches | lr 0.0000 | ms/batch 582.51 | loss  0.36 | cls  0.36 | err  0.12 | \n",
      "scGPT - INFO - | epoch  26 | 400/1900 batches | lr 0.0000 | ms/batch 580.68 | loss  0.38 | cls  0.38 | err  0.12 | \n",
      "scGPT - INFO - | epoch  26 | 500/1900 batches | lr 0.0000 | ms/batch 581.53 | loss  0.37 | cls  0.37 | err  0.13 | \n",
      "scGPT - INFO - | epoch  26 | 600/1900 batches | lr 0.0000 | ms/batch 580.53 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  26 | 700/1900 batches | lr 0.0000 | ms/batch 580.38 | loss  0.39 | cls  0.39 | err  0.13 | \n",
      "scGPT - INFO - | epoch  26 | 800/1900 batches | lr 0.0000 | ms/batch 580.37 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  26 | 900/1900 batches | lr 0.0000 | ms/batch 581.79 | loss  0.42 | cls  0.42 | err  0.15 | \n",
      "scGPT - INFO - | epoch  26 | 1000/1900 batches | lr 0.0000 | ms/batch 580.31 | loss  0.37 | cls  0.37 | err  0.13 | \n",
      "scGPT - INFO - | epoch  26 | 1100/1900 batches | lr 0.0000 | ms/batch 580.26 | loss  0.39 | cls  0.39 | err  0.13 | \n",
      "scGPT - INFO - | epoch  26 | 1200/1900 batches | lr 0.0000 | ms/batch 580.39 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  26 | 1300/1900 batches | lr 0.0000 | ms/batch 580.42 | loss  0.37 | cls  0.37 | err  0.12 | \n",
      "scGPT - INFO - | epoch  26 | 1400/1900 batches | lr 0.0000 | ms/batch 580.36 | loss  0.40 | cls  0.40 | err  0.15 | \n",
      "scGPT - INFO - | epoch  26 | 1500/1900 batches | lr 0.0000 | ms/batch 582.73 | loss  0.40 | cls  0.40 | err  0.14 | \n",
      "scGPT - INFO - | epoch  26 | 1600/1900 batches | lr 0.0000 | ms/batch 580.27 | loss  0.43 | cls  0.43 | err  0.15 | \n",
      "scGPT - INFO - | epoch  26 | 1700/1900 batches | lr 0.0000 | ms/batch 580.24 | loss  0.40 | cls  0.40 | err  0.14 | \n",
      "scGPT - INFO - | epoch  26 | 1800/1900 batches | lr 0.0000 | ms/batch 580.23 | loss  0.38 | cls  0.38 | err  0.13 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  26 | time: 1152.89s | valid loss/mse 0.4520 | err 0.1500\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  27, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  27 | 100/1900 batches | lr 0.0000 | ms/batch 592.44 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - | epoch  27 | 200/1900 batches | lr 0.0000 | ms/batch 580.48 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  27 | 300/1900 batches | lr 0.0000 | ms/batch 580.57 | loss  0.37 | cls  0.37 | err  0.12 | \n",
      "scGPT - INFO - | epoch  27 | 400/1900 batches | lr 0.0000 | ms/batch 580.46 | loss  0.37 | cls  0.37 | err  0.12 | \n",
      "scGPT - INFO - | epoch  27 | 500/1900 batches | lr 0.0000 | ms/batch 582.63 | loss  0.37 | cls  0.37 | err  0.12 | \n",
      "scGPT - INFO - | epoch  27 | 600/1900 batches | lr 0.0000 | ms/batch 581.39 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  27 | 700/1900 batches | lr 0.0000 | ms/batch 580.80 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - | epoch  27 | 800/1900 batches | lr 0.0000 | ms/batch 580.58 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  27 | 900/1900 batches | lr 0.0000 | ms/batch 582.12 | loss  0.42 | cls  0.42 | err  0.15 | \n",
      "scGPT - INFO - | epoch  27 | 1000/1900 batches | lr 0.0000 | ms/batch 580.54 | loss  0.37 | cls  0.37 | err  0.13 | \n",
      "scGPT - INFO - | epoch  27 | 1100/1900 batches | lr 0.0000 | ms/batch 580.43 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - | epoch  27 | 1200/1900 batches | lr 0.0000 | ms/batch 581.80 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  27 | 1300/1900 batches | lr 0.0000 | ms/batch 580.58 | loss  0.37 | cls  0.37 | err  0.12 | \n",
      "scGPT - INFO - | epoch  27 | 1400/1900 batches | lr 0.0000 | ms/batch 580.57 | loss  0.40 | cls  0.40 | err  0.14 | \n",
      "scGPT - INFO - | epoch  27 | 1500/1900 batches | lr 0.0000 | ms/batch 580.77 | loss  0.39 | cls  0.39 | err  0.13 | \n",
      "scGPT - INFO - | epoch  27 | 1600/1900 batches | lr 0.0000 | ms/batch 582.90 | loss  0.43 | cls  0.43 | err  0.15 | \n",
      "scGPT - INFO - | epoch  27 | 1700/1900 batches | lr 0.0000 | ms/batch 580.58 | loss  0.40 | cls  0.40 | err  0.14 | \n",
      "scGPT - INFO - | epoch  27 | 1800/1900 batches | lr 0.0000 | ms/batch 581.83 | loss  0.38 | cls  0.38 | err  0.12 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  27 | time: 1153.45s | valid loss/mse 0.4499 | err 0.1500\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.4499\n",
      "random masking at epoch  28, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  28 | 100/1900 batches | lr 0.0000 | ms/batch 592.58 | loss  0.39 | cls  0.39 | err  0.13 | \n",
      "scGPT - INFO - | epoch  28 | 200/1900 batches | lr 0.0000 | ms/batch 580.22 | loss  0.42 | cls  0.42 | err  0.15 | \n",
      "scGPT - INFO - | epoch  28 | 300/1900 batches | lr 0.0000 | ms/batch 582.29 | loss  0.37 | cls  0.37 | err  0.12 | \n",
      "scGPT - INFO - | epoch  28 | 400/1900 batches | lr 0.0000 | ms/batch 580.30 | loss  0.38 | cls  0.38 | err  0.12 | \n",
      "scGPT - INFO - | epoch  28 | 500/1900 batches | lr 0.0000 | ms/batch 580.24 | loss  0.37 | cls  0.37 | err  0.12 | \n",
      "scGPT - INFO - | epoch  28 | 600/1900 batches | lr 0.0000 | ms/batch 582.12 | loss  0.42 | cls  0.42 | err  0.14 | \n",
      "scGPT - INFO - | epoch  28 | 700/1900 batches | lr 0.0000 | ms/batch 581.56 | loss  0.39 | cls  0.39 | err  0.13 | \n",
      "scGPT - INFO - | epoch  28 | 800/1900 batches | lr 0.0000 | ms/batch 580.58 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  28 | 900/1900 batches | lr 0.0000 | ms/batch 580.76 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  28 | 1000/1900 batches | lr 0.0000 | ms/batch 582.33 | loss  0.37 | cls  0.37 | err  0.13 | \n",
      "scGPT - INFO - | epoch  28 | 1100/1900 batches | lr 0.0000 | ms/batch 580.65 | loss  0.39 | cls  0.39 | err  0.13 | \n",
      "scGPT - INFO - | epoch  28 | 1200/1900 batches | lr 0.0000 | ms/batch 580.60 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - | epoch  28 | 1300/1900 batches | lr 0.0000 | ms/batch 582.13 | loss  0.37 | cls  0.37 | err  0.12 | \n",
      "scGPT - INFO - | epoch  28 | 1400/1900 batches | lr 0.0000 | ms/batch 580.63 | loss  0.39 | cls  0.39 | err  0.14 | \n",
      "scGPT - INFO - | epoch  28 | 1500/1900 batches | lr 0.0000 | ms/batch 580.75 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - | epoch  28 | 1600/1900 batches | lr 0.0000 | ms/batch 580.90 | loss  0.43 | cls  0.43 | err  0.15 | \n",
      "scGPT - INFO - | epoch  28 | 1700/1900 batches | lr 0.0000 | ms/batch 583.25 | loss  0.40 | cls  0.40 | err  0.14 | \n",
      "scGPT - INFO - | epoch  28 | 1800/1900 batches | lr 0.0000 | ms/batch 580.62 | loss  0.38 | cls  0.38 | err  0.12 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  28 | time: 1153.61s | valid loss/mse 0.4475 | err 0.1487\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.4475\n",
      "random masking at epoch  29, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  29 | 100/1900 batches | lr 0.0000 | ms/batch 592.65 | loss  0.38 | cls  0.38 | err  0.13 | \n",
      "scGPT - INFO - | epoch  29 | 200/1900 batches | lr 0.0000 | ms/batch 582.78 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  29 | 300/1900 batches | lr 0.0000 | ms/batch 580.61 | loss  0.37 | cls  0.37 | err  0.12 | \n",
      "scGPT - INFO - | epoch  29 | 400/1900 batches | lr 0.0000 | ms/batch 581.93 | loss  0.37 | cls  0.37 | err  0.12 | \n",
      "scGPT - INFO - | epoch  29 | 500/1900 batches | lr 0.0000 | ms/batch 580.45 | loss  0.37 | cls  0.37 | err  0.12 | \n",
      "scGPT - INFO - | epoch  29 | 600/1900 batches | lr 0.0000 | ms/batch 581.95 | loss  0.41 | cls  0.41 | err  0.14 | \n",
      "scGPT - INFO - | epoch  29 | 700/1900 batches | lr 0.0000 | ms/batch 580.63 | loss  0.39 | cls  0.39 | err  0.12 | \n",
      "scGPT - INFO - | epoch  29 | 800/1900 batches | lr 0.0000 | ms/batch 581.55 | loss  0.42 | cls  0.42 | err  0.14 | \n",
      "scGPT - INFO - | epoch  29 | 900/1900 batches | lr 0.0000 | ms/batch 580.67 | loss  0.41 | cls  0.41 | err  0.15 | \n",
      "scGPT - INFO - | epoch  29 | 1000/1900 batches | lr 0.0000 | ms/batch 580.43 | loss  0.36 | cls  0.36 | err  0.13 | \n",
      "scGPT - INFO - | epoch  29 | 1100/1900 batches | lr 0.0000 | ms/batch 580.40 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - | epoch  29 | 1200/1900 batches | lr 0.0000 | ms/batch 580.35 | loss  0.40 | cls  0.40 | err  0.14 | \n",
      "scGPT - INFO - | epoch  29 | 1300/1900 batches | lr 0.0000 | ms/batch 581.82 | loss  0.37 | cls  0.37 | err  0.12 | \n",
      "scGPT - INFO - | epoch  29 | 1400/1900 batches | lr 0.0000 | ms/batch 580.36 | loss  0.40 | cls  0.40 | err  0.14 | \n",
      "scGPT - INFO - | epoch  29 | 1500/1900 batches | lr 0.0000 | ms/batch 580.60 | loss  0.39 | cls  0.39 | err  0.13 | \n",
      "scGPT - INFO - | epoch  29 | 1600/1900 batches | lr 0.0000 | ms/batch 580.47 | loss  0.43 | cls  0.43 | err  0.14 | \n",
      "scGPT - INFO - | epoch  29 | 1700/1900 batches | lr 0.0000 | ms/batch 580.46 | loss  0.39 | cls  0.39 | err  0.14 | \n",
      "scGPT - INFO - | epoch  29 | 1800/1900 batches | lr 0.0000 | ms/batch 581.38 | loss  0.38 | cls  0.38 | err  0.12 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  29 | time: 1153.21s | valid loss/mse 0.4485 | err 0.1484\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  30, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  30 | 100/1900 batches | lr 0.0000 | ms/batch 594.29 | loss  0.38 | cls  0.38 | err  0.13 | \n",
      "scGPT - INFO - | epoch  30 | 200/1900 batches | lr 0.0000 | ms/batch 580.12 | loss  0.42 | cls  0.42 | err  0.14 | \n",
      "scGPT - INFO - | epoch  30 | 300/1900 batches | lr 0.0000 | ms/batch 581.72 | loss  0.36 | cls  0.36 | err  0.12 | \n",
      "scGPT - INFO - | epoch  30 | 400/1900 batches | lr 0.0000 | ms/batch 580.36 | loss  0.37 | cls  0.37 | err  0.12 | \n",
      "scGPT - INFO - | epoch  30 | 500/1900 batches | lr 0.0000 | ms/batch 580.39 | loss  0.37 | cls  0.37 | err  0.12 | \n",
      "scGPT - INFO - | epoch  30 | 600/1900 batches | lr 0.0000 | ms/batch 581.86 | loss  0.42 | cls  0.42 | err  0.14 | \n",
      "scGPT - INFO - | epoch  30 | 700/1900 batches | lr 0.0000 | ms/batch 580.60 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - | epoch  30 | 800/1900 batches | lr 0.0000 | ms/batch 581.97 | loss  0.42 | cls  0.42 | err  0.14 | \n",
      "scGPT - INFO - | epoch  30 | 900/1900 batches | lr 0.0000 | ms/batch 581.49 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - | epoch  30 | 1000/1900 batches | lr 0.0000 | ms/batch 580.46 | loss  0.37 | cls  0.37 | err  0.13 | \n",
      "scGPT - INFO - | epoch  30 | 1100/1900 batches | lr 0.0000 | ms/batch 581.83 | loss  0.39 | cls  0.39 | err  0.13 | \n",
      "scGPT - INFO - | epoch  30 | 1200/1900 batches | lr 0.0000 | ms/batch 580.54 | loss  0.39 | cls  0.39 | err  0.13 | \n",
      "scGPT - INFO - | epoch  30 | 1300/1900 batches | lr 0.0000 | ms/batch 582.10 | loss  0.35 | cls  0.35 | err  0.12 | \n",
      "scGPT - INFO - | epoch  30 | 1400/1900 batches | lr 0.0000 | ms/batch 580.46 | loss  0.40 | cls  0.40 | err  0.14 | \n",
      "scGPT - INFO - | epoch  30 | 1500/1900 batches | lr 0.0000 | ms/batch 581.98 | loss  0.40 | cls  0.40 | err  0.13 | \n",
      "scGPT - INFO - | epoch  30 | 1600/1900 batches | lr 0.0000 | ms/batch 580.49 | loss  0.42 | cls  0.42 | err  0.15 | \n",
      "scGPT - INFO - | epoch  30 | 1700/1900 batches | lr 0.0000 | ms/batch 580.41 | loss  0.40 | cls  0.40 | err  0.14 | \n",
      "scGPT - INFO - | epoch  30 | 1800/1900 batches | lr 0.0000 | ms/batch 581.77 | loss  0.37 | cls  0.37 | err  0.12 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  30 | time: 1153.73s | valid loss/mse 0.4468 | err 0.1487\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.4468\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_avg_bio = 0.0\n",
    "best_model = None\n",
    "define_wandb_metrcis()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    if config.do_train:\n",
    "        train(\n",
    "            model,\n",
    "            loader=train_loader,\n",
    "        )\n",
    "    val_loss, val_err = evaluate(\n",
    "        model,\n",
    "        loader=valid_loader,\n",
    "    )\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\n",
    "        f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "        f\"valid loss/mse {val_loss:5.4f} | err {val_err:5.4f}\"\n",
    "    )\n",
    "    logger.info(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    if DAB_separate_optim:\n",
    "        scheduler_dab.step()\n",
    "    if ADV:\n",
    "        scheduler_D.step()\n",
    "        scheduler_E.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a7337c",
   "metadata": {},
   "source": [
    "## Step 5: Inference with fine-tuned scGPT model\n",
    "In the cell-type annotation task, the fine-tuned scGPT predicts cell-type labels for query set as inference. The model performance is evaluated on standard classificaton metrics. Here we visualize the predicted labels over the scGPT cell embeddings, and present the confusion matrix for detailed classification performance on the cell-group level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52cca76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% inference\n",
    "def test(model: nn.Module, annDataObj: DataLoader) -> float:\n",
    "    all_counts = (\n",
    "        annDataObj.layers[input_layer_key].A\n",
    "        if issparse(annDataObj.layers[input_layer_key])\n",
    "        else annDataObj.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "    celltypes_labels = annDataObj.obs[\"celltype_id\"].tolist()  # make sure count from 0\n",
    "    celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "    batch_ids = annDataObj.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    tokenized_test = tokenize_and_pad_batch(\n",
    "        all_counts,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=include_zero_gene,\n",
    "    )\n",
    "\n",
    "    input_values_test = random_mask_value(\n",
    "        tokenized_test[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "\n",
    "    test_data_pt = {\n",
    "        \"gene_ids\": tokenized_test[\"genes\"],\n",
    "        \"values\": input_values_test,\n",
    "        \"target_values\": tokenized_test[\"values\"],\n",
    "        \"batch_labels\": torch.from_numpy(batch_ids).long(),\n",
    "        \"celltype_labels\": torch.from_numpy(celltypes_labels).long(),\n",
    "    }\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=SeqDataset(test_data_pt),\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=min(len(os.sched_getaffinity(0)), eval_batch_size // 2),\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    predictions = evaluate(\n",
    "        model,\n",
    "        loader=test_loader,\n",
    "        return_raw=True,\n",
    "    )\n",
    "\n",
    "    # compute accuracy, precision, recall, f1\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "    accuracy = accuracy_score(celltypes_labels, predictions)\n",
    "    precision = precision_score(celltypes_labels, predictions, average=\"macro\")\n",
    "    recall = recall_score(celltypes_labels, predictions, average=\"macro\")\n",
    "    macro_f1 = f1_score(celltypes_labels, predictions, average=\"macro\")\n",
    "\n",
    "    logger.info(\n",
    "        f\"Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, \"\n",
    "        f\"Macro F1: {macro_f1:.3f}\"\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        \"test/accuracy\": accuracy,\n",
    "        \"test/precision\": precision,\n",
    "        \"test/recall\": recall,\n",
    "        \"test/macro_f1\": macro_f1,\n",
    "    }\n",
    "\n",
    "    return predictions, celltypes_labels, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f46e7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Accuracy: 0.858, Precision: 0.663, Recall: 0.623, Macro F1: 0.622\n"
     ]
    }
   ],
   "source": [
    "predictions, labels, results = test(best_model, adata_test)\n",
    "adata_test_raw.obs[\"predictions\"] = [id2type[p] for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea46ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_test_raw.write_h5ad(data_dir/\"predictionsNikolic_10pc_60_ep_lr10-5.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029109c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "torch.save(best_model.state_dict(), save_dir / \"nikolic_best_model_10pc_60_ep_lr10-5.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
